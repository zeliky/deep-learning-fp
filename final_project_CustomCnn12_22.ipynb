{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zeliky/deep-learning-fp/blob/main/final_project_CustomCnn12_22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KHjMiX4CNEZM"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "from scipy.io import loadmat\n",
        "import numpy as np\n",
        "from PIL import Image,ImageShow\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "from tensorflow.keras.models import Sequential, Model,load_model\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D,Lambda, Flatten, Dense, Dropout,Activation, BatchNormalization,Add, GlobalAveragePooling2D\n",
        "from tensorflow.keras.utils import Sequence, to_categorical, plot_model\n",
        "from keras.regularizers import l2\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from tensorflow.keras.metrics import Mean\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras import backend as K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-iV4tY5kNOsL",
        "outputId": "a288234e-2722-497b-9f80-371fb75c6c8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/gdrive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2n6KHr3aNZ9t"
      },
      "outputs": [],
      "source": [
        "BASE_PATH  = '/gdrive/MyDrive/deep-learning/final-project/data-sets/'\n",
        "ORIGINAL_IMAGES = BASE_PATH + \"0_Images/\"\n",
        "ROTATED_IMAGES =  BASE_PATH + \"1_ImagesRotated/\"\n",
        "MEDIAN_BW_IMAGES =  BASE_PATH + \"2_ImagesMedianBW/\"\n",
        "LINES_REMOVED_BW_IMAGES =  BASE_PATH + \"3_ImagesLinesRemovedBW/\"\n",
        "LINES_REMOVED_IMAGES =  BASE_PATH + \"4_ImagesLinesRemoved/\"\n",
        "DARK_LINES =  BASE_PATH + \"5_DataDarkLines/\"\n",
        "SAMPLE_FILE_PATTERN = \"lines{}_Page_{}.jpg\"\n",
        "METADATA_PATTERN = \"lines{}_Page_{}.mat\"\n",
        "MODEL_CHECKPOINT_PATH = BASE_PATH + \"model_checkpoints/\"\n",
        "\n",
        "NUMPY_STORAGE=  BASE_PATH +\"numpy_storage/{}_train_validation.npz\"\n",
        "ALLOWED_TYPES = [ORIGINAL_IMAGES,ROTATED_IMAGES, MEDIAN_BW_IMAGES, LINES_REMOVED_BW_IMAGES, LINES_REMOVED_IMAGES]\n",
        "TRAIN_TYPES =  [ LINES_REMOVED_BW_IMAGES, LINES_REMOVED_IMAGES]\n",
        "VALIDATE_TYPES =  [ ROTATED_IMAGES, MEDIAN_BW_IMAGES]\n",
        "LINE_SHAPE = (225, 4965)\n",
        "INPUT_SQUARE = (227, 227)\n",
        "\n",
        "MODE_TRAIN = 'train'\n",
        "MODE_VALIDATION = 'validation'\n",
        "MODE_TEST = 'test'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ysVzTENyP2qm"
      },
      "outputs": [],
      "source": [
        "class MetaData:\n",
        "  @staticmethod\n",
        "  def from_mat_data(data):\n",
        "    metadata = MetaData()\n",
        "    SCALE_FACTOR = data['SCALE_FACTOR'].flatten()[0]\n",
        "    peaks_indices = data['peaks_indices'].flatten()\n",
        "    metadata.min_y  = SCALE_FACTOR * min(peaks_indices)\n",
        "    metadata.max_y  = SCALE_FACTOR *  max(peaks_indices)\n",
        "    metadata.peaks_indices = [SCALE_FACTOR*a - metadata.min_y for a in  peaks_indices]\n",
        "    metadata.index_of_max_in_peak_indices = data['index_of_max_in_peak_indices'].flatten()[0]\n",
        "    metadata.delta = data['delta'].flatten()[0]\n",
        "    metadata.top_test_area = data['top_test_area'].flatten()[0] - metadata.min_y\n",
        "    metadata.bottom_test_area = data['bottom_test_area'].flatten()[0] - metadata.min_y\n",
        "    metadata.total_lines = len(metadata.peaks_indices)\n",
        "    return metadata\n",
        "\n",
        "  def __repr__(self):\n",
        "    return str(self.__dict__)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelOptions:\n",
        "    def __init__(self, **kwargs):\n",
        "        self.num_classes = kwargs.get('num_classes', 10)\n",
        "        self.batch_size = kwargs.get('batch_size', 100)\n",
        "        self.image_height = kwargs.get('image_height', 150)\n",
        "        self.image_width = kwargs.get('image_width', 150)\n",
        "        self.num_channels = kwargs.get('num_channels', 1)\n",
        "        self.max_sequence_length = kwargs.get('max_sequence_length', 40)\n",
        "        self.random_shuffle_amount = kwargs.get('random_shuffle_amount', 5)\n",
        "        #self.lstm_units = kwargs.get('lstm_units', 5)\n",
        "        self.max_embedding_samples = kwargs.get('max_embedding_samples', 5)\n",
        "        self.alpha = kwargs.get('alpha', 0.2)\n",
        "        self.embedding_dim = kwargs.get('embedding_dim', 512)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self.__dict__)"
      ],
      "metadata": {
        "id": "SGfTdvsq4cQK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "oVWYP76qw-ZI"
      },
      "outputs": [],
      "source": [
        "class DataSet:\n",
        "    def __init__(self):\n",
        "        self.user_files = {}\n",
        "        self.metadata = {}\n",
        "        self._build_index()\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.images = {}\n",
        "\n",
        "    def load_image(self, path, user_id):\n",
        "        bin, id = self.user_files[user_id]\n",
        "        image_type = self._image_type(path)\n",
        "\n",
        "        file_name = SAMPLE_FILE_PATTERN.format(bin, id)\n",
        "        image_path = \"{}{}\".format(path, file_name)\n",
        "        image_metadata = self._get_cached_metadata(user_id)\n",
        "        if image_metadata is None:\n",
        "            image_metadata = self.image_metadata(user_id)\n",
        "            self._cache_metadata(user_id, image_metadata)\n",
        "\n",
        "        user_file = self._get_cached_image(image_type, user_id)\n",
        "        if user_file is None:\n",
        "            print('loading image from {}'.format(image_path))\n",
        "            with open(image_path, 'r') as f:\n",
        "                im = Image.open(image_path)\n",
        "                user_file = UserFile(im, image_metadata)\n",
        "                self._cache_image(image_type, user_id, user_file)\n",
        "        return user_file\n",
        "\n",
        "    def image_metadata(self, user_id):\n",
        "        bin, id = self.user_files[user_id]\n",
        "        file_name = METADATA_PATTERN.format(bin, id)\n",
        "        metadata_path = \"{}{}\".format(DARK_LINES, file_name)\n",
        "        # print(metadata_path)\n",
        "        data = loadmat(metadata_path)\n",
        "        return MetaData.from_mat_data(data)\n",
        "\n",
        "    def _image_type(self, path):\n",
        "        reg = re.search(r'\\d+', path)\n",
        "        return int(reg.group())\n",
        "\n",
        "\n",
        "    def _build_index(self):\n",
        "        directory_files = os.listdir(ORIGINAL_IMAGES)\n",
        "        for idx, fname in enumerate(directory_files):\n",
        "            pattern = re.search(r'lines(\\d+)_Page_(\\d+)', fname.replace(BASE_PATH, ''))\n",
        "            self.user_files[idx] = (pattern.group(1), pattern.group(2))\n",
        "\n",
        "    def _get_cached_image(self, image_type, user_id):\n",
        "        if image_type in self.images and user_id in self.images[image_type]:\n",
        "            return self.images[image_type][user_id]\n",
        "        return None\n",
        "\n",
        "    def _cache_image(self, image_type, user_id, image):\n",
        "        if image_type not in self.images:\n",
        "            self.images[image_type] = {}\n",
        "        if user_id not in self.images[image_type]:\n",
        "            self.images[image_type][user_id] = image\n",
        "\n",
        "    def _get_cached_metadata(self, user_id):\n",
        "        if user_id in self.metadata:\n",
        "            return self.metadata[user_id]\n",
        "        return None\n",
        "\n",
        "    def _cache_metadata(self, user_id, metadata: MetaData):\n",
        "        self.metadata[user_id] = metadata\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------\n",
        "#should be kept global so it will be shared by multi processes\n",
        "full_data_set = DataSet()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "zm_2xR71xR1n"
      },
      "outputs": [],
      "source": [
        "class UserFile:\n",
        "    def __init__(self, im, metadata):\n",
        "        # image_arr = np.asarray(im.getchannel(0).getdata())\n",
        "        self.im = im.copy()\n",
        "        if im.mode != 'L':\n",
        "            im = im.convert(mode='L')\n",
        "        image_arr = np.asarray(im.getchannel(0))\n",
        "        # image_arr = image_arr.reshape(im.height, im.width)\n",
        "        # print(image_arr.shape)\n",
        "        self.data = image_arr[metadata.min_y: metadata.max_y, :]\n",
        "        self.metadata = metadata\n",
        "\n",
        "    def get_testing_line(self):\n",
        "        y_range = (self.metadata.top_test_area, self.metadata.bottom_test_area)\n",
        "        return self.data[y_range[0]:y_range[1], :]\n",
        "\n",
        "    def get_test_line_idx(self):\n",
        "        for i in range(1, self.metadata.total_lines):\n",
        "            if self.is_test_line(i):\n",
        "                return i\n",
        "\n",
        "    def get_all_lines(self, threshold=2000):\n",
        "        for i in range(1, self.metadata.total_lines):\n",
        "            yield self.get_line(i)\n",
        "\n",
        "    def get_line(self, line_idx):\n",
        "        y_range = (self.metadata.peaks_indices[line_idx - 1:line_idx + 1])\n",
        "        return self.data[y_range[0]:y_range[1] + 30, :]\n",
        "\n",
        "    def is_test_line(self, line_idx):\n",
        "        bounderies = self.metadata.peaks_indices[line_idx - 1:line_idx + 1]\n",
        "        return abs(int(bounderies[0]) - self.metadata.top_test_area) < 50 and abs(\n",
        "            int(bounderies[1]) - self.metadata.bottom_test_area) < 50\n",
        "\n",
        "    def show(self):\n",
        "        image = Image.fromarray(self.data.astype(np.uint8))\n",
        "        image.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7CIxxCGdR1uV"
      },
      "outputs": [],
      "source": [
        "class UserDataset:\n",
        "    def __init__(self, user_id):\n",
        "        self.user_id = user_id\n",
        "        self.train_lines = []\n",
        "        self.validation_lines = []\n",
        "        self.all_lines = []\n",
        "        self.test_line = None\n",
        "        self.split_points = {}\n",
        "        self.min_width = 20\n",
        "        self.min_colored_pixels = 500 * 255\n",
        "\n",
        "    def warmup(self):\n",
        "        e = ThreadPoolExecutor(max_workers=len(ALLOWED_TYPES))\n",
        "        futures = [e.submit(full_data_set.load_image, t, self.user_id) for t in ALLOWED_TYPES]\n",
        "        results = [f.result() for f in futures]\n",
        "        self.split_dataset()\n",
        "\n",
        "    def split_dataset(self, train_split=0.8):\n",
        "        bw_image = full_data_set.load_image(LINES_REMOVED_BW_IMAGES, self.user_id)\n",
        "        self.train_lines, self.validation_lines = select_train_validation_lines(bw_image)\n",
        "        self.test_line = bw_image.get_test_line_idx()\n",
        "        self.all_lines = sorted(self.train_lines + self.validation_lines)\n",
        "\n",
        "    def get_letters(self, img_path, line_idx, target_size):\n",
        "        split_points = self._get_characters_split_points(line_idx)\n",
        "        user_file = full_data_set.load_image(img_path, self.user_id)\n",
        "        line = normalized_line(user_file.get_line(line_idx))\n",
        "        for (x, y, w, h) in split_points:\n",
        "            img = line[:, x:x + w]\n",
        "            # print(f\"get_letter shape {img.shape}\")\n",
        "            thumbnail = create_thumbnail(img, target_size)\n",
        "            np_im = np.array(thumbnail, dtype=np.float32) / 255\n",
        "            np_img = np_im.reshape(target_size[0], target_size[1], 1)\n",
        "            yield np_img\n",
        "        return\n",
        "\n",
        "    def get_line_as_sequence(self, img_path, line_idx, max_sequence_length, target_size):\n",
        "        sequence = []\n",
        "        user_file = full_data_set.load_image(img_path, self.user_id)\n",
        "        line = normalized_line(user_file.get_line(line_idx))\n",
        "        split_points = self._get_characters_split_points(line_idx)\n",
        "        for (x, y, w, h) in split_points:\n",
        "            img = line[:, x:x + w]\n",
        "            thumbnail = create_thumbnail(img, target_size)\n",
        "            np_im = np.array(thumbnail, dtype=np.float32) / 255\n",
        "            np_img = np_im.reshape(target_size[0], target_size[1], 1)\n",
        "            sequence.append(np_img)\n",
        "        return pad_sequence(max_sequence_length, sequence, target_size[0], target_size[1], 1)\n",
        "\n",
        "    def random_line_generator(self, mode, max_sequence_length, target_size, sample_from_lines_amount=None,\n",
        "                              sequence_length=None, original_only=False):\n",
        "        while True:\n",
        "            types = ALLOWED_TYPES if not original_only else [ORIGINAL_IMAGES]\n",
        "            img_path = random.choice(types)\n",
        "            user_file = full_data_set.load_image(img_path, self.user_id)\n",
        "\n",
        "            lines = self._get_lines_ids_set(mode)\n",
        "            if sample_from_lines_amount is None:\n",
        "                sample_from_lines_amount = random.randint(1, len(lines) - 1)\n",
        "            selected_lines = random.sample(lines, sample_from_lines_amount)\n",
        "            if sequence_length is None:\n",
        "                sequence_length = random.randint(int(0.3 * max_sequence_length), max_sequence_length)\n",
        "            sequence = []\n",
        "            for _ in range(sequence_length):\n",
        "                line_idx = random.choice(selected_lines)\n",
        "                line = normalized_line(user_file.get_line(line_idx))\n",
        "                split_points = self._get_characters_split_points(line_idx)\n",
        "                (x, y, w, h) = random.choice(split_points)\n",
        "                img = line[:, x:x + w]\n",
        "                thumbnail = create_thumbnail(img, target_size)\n",
        "                np_im = np.array(thumbnail, dtype=np.float32) / 255\n",
        "                np_img = np_im.reshape(target_size[0], target_size[1], 1)\n",
        "                sequence.append(np_img)\n",
        "            yield pad_sequence(max_sequence_length, sequence, target_size[0], target_size[1], 1)\n",
        "\n",
        "    def random_letters_generator(self, mode, target_size, random_shuffle_amount=1, original_only=False):\n",
        "        while True:\n",
        "            types = ALLOWED_TYPES if not original_only else [ORIGINAL_IMAGES]\n",
        "            img_path = random.choice(types)\n",
        "            user_file = full_data_set.load_image(img_path, self.user_id)\n",
        "\n",
        "            lines = self._get_lines_ids_set(mode)\n",
        "            line_idx = random.choice(lines)\n",
        "            line = normalized_line(user_file.get_line(line_idx))\n",
        "            split_points = self._get_characters_split_points(line_idx)\n",
        "            split_index = random.randint(0, len(split_points) - 1)\n",
        "            (x, y, w, h) = split_points[split_index]\n",
        "\n",
        "            img = line[:, x:x + w]\n",
        "            thumbnails = [create_thumbnail(img, target_size) for _ in range(random_shuffle_amount)]\n",
        "            for i, thumbnail in enumerate(thumbnails):\n",
        "                np_im = np.array(thumbnail, dtype=np.float32) / 255\n",
        "                np_img = np_im.reshape(target_size[0], target_size[1], 1)\n",
        "                # print(f\"{img_path}: u:{self.user_id} l:{line_idx} x:{x}-{x+w} rand:{i}\")\n",
        "                yield np_img, img_path, line_idx, split_index\n",
        "\n",
        "    def get_letter(self, img_path, line_idx, split_index, target_size):\n",
        "        user_file = full_data_set.load_image(img_path, self.user_id)\n",
        "        line = normalized_line(user_file.get_line(line_idx))\n",
        "        split_points = self._get_characters_split_points(line_idx)\n",
        "        (x, y, w, h) = split_points[split_index]\n",
        "        img = line[:, x:x + w]\n",
        "        thumbnail = create_thumbnail(img, target_size)\n",
        "        np_im = np.array(thumbnail, dtype=np.float32) / 255\n",
        "        return np_im.reshape(target_size[0], target_size[1], 1)\n",
        "\n",
        "    def _get_lines_ids_set(self, mode):\n",
        "        if mode == MODE_TRAIN:\n",
        "            return self.train_lines\n",
        "        elif mode == MODE_VALIDATION:\n",
        "            return self.validation_lines\n",
        "        return [self.test_line]\n",
        "\n",
        "    def _get_characters_split_points(self, idx):\n",
        "        if idx in self.split_points:\n",
        "            return self.split_points[idx]\n",
        "        img = full_data_set.load_image(LINES_REMOVED_BW_IMAGES, self.user_id)\n",
        "        line = normalized_line(img.get_line(idx))\n",
        "        binary = np.where(line > 30, 1, 0).astype('uint8')\n",
        "        rect_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (8, 8))\n",
        "        dilation = cv2.dilate(binary, rect_kernel, iterations=1)\n",
        "        contours, hierarchy = cv2.findContours(dilation, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "        split_points = []\n",
        "        for cnt in contours:\n",
        "            x, y, w, h = cv2.boundingRect(cnt)\n",
        "            if w > self.min_width:\n",
        "                sub_img = line[:, x:x + w]\n",
        "                # print(f\"_get_characters_split_points line \\t {idx}\\t{x}\\t{x+w}\\t{sub_img.sum()}\")\n",
        "                if sub_img.sum() > self.min_colored_pixels:\n",
        "                    split_points.append((x, y, w, h))\n",
        "\n",
        "        self.split_points[idx] = sorted(split_points, key=lambda tup: tup[0])\n",
        "        # print( self.split_points[idx])\n",
        "        return self.split_points[idx]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "d3bYCMFTUWyy"
      },
      "outputs": [],
      "source": [
        "def image_dots(img_data, threshold=50):\n",
        "    height, width = img_data.shape\n",
        "    for i in range(height):\n",
        "        line = ''\n",
        "        for j in range(width):\n",
        "            if img_data[i, j] < threshold:\n",
        "                line += ' '\n",
        "            else:\n",
        "                line += '.'\n",
        "        print(line)\n",
        "\n",
        "\n",
        "def show_line(line_data):\n",
        "    image = Image.fromarray(line_data.astype(np.uint8))\n",
        "    image.show()\n",
        "\n",
        "\n",
        "def is_empty_line(line_data, threshold=5000):\n",
        "    values = line_data.flatten()\n",
        "    sum = values[values < 50].sum()\n",
        "    # print(\"is_empty_line {}\".format(sum))\n",
        "    return sum < threshold\n",
        "\n",
        "\n",
        "def normalized_line(line_data):\n",
        "    desired_shape = LINE_SHAPE\n",
        "    # normalized_data =  (255 - line_data) / 255.0\n",
        "    normalized_data = (255 - line_data)\n",
        "    pad_rows = max(0, desired_shape[0] - normalized_data.shape[0])\n",
        "    pad_cols = max(0, desired_shape[1] - normalized_data.shape[1])\n",
        "\n",
        "    padded_array = np.pad(normalized_data, ((0, pad_rows), (0, pad_cols)), mode='constant')\n",
        "    return padded_array\n",
        "\n",
        "\n",
        "def select_train_validation_lines(user_image, train_split=0.8):\n",
        "    rows = []\n",
        "    for i, line in enumerate(user_image.get_all_lines()):\n",
        "        idx = i + 1\n",
        "        if not user_image.is_test_line(idx) and not is_empty_line(line):\n",
        "            rows.append(idx)\n",
        "\n",
        "    random.shuffle(rows)\n",
        "    split_idx = int(len(rows) * train_split)\n",
        "    #print((rows[0:split_idx], rows[split_idx:]))\n",
        "    return (rows[0:split_idx], rows[split_idx:])\n",
        "\n",
        "\n",
        "def split_and_shuffle_array(arr, split_points):\n",
        "    chunks = split_array(arr, split_points)\n",
        "    np.random.shuffle(chunks)\n",
        "    shuffled_array = np.concatenate(chunks, axis=1)\n",
        "    return shuffled_array\n",
        "\n",
        "\n",
        "def split_array(arr, split_points):\n",
        "    chunks = []\n",
        "    start_idx = 0\n",
        "    for end_idx in split_points:\n",
        "        chunk = arr[:, start_idx:end_idx]\n",
        "        chunks.append(chunk)\n",
        "        start_idx = end_idx\n",
        "    last_chunk = arr[:, start_idx:]\n",
        "    chunks.append(last_chunk)\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def create_thumbnail(image_array, target_size):\n",
        "    height, width = image_array.shape\n",
        "    target_height, target_width = target_size\n",
        "    org_image = Image.fromarray(image_array)\n",
        "\n",
        "    random_scale_w = random.uniform(0.8, 1.2)\n",
        "    random_scale_h = random.uniform(0.8, 1.2)\n",
        "    random_rotate = random.randint(-15, 15)\n",
        "    org_image_rs = org_image.resize((int(width * random_scale_w), int(height * random_scale_h)), Image.NEAREST)\n",
        "    org_image_ro = org_image_rs.rotate(random_rotate, Image.NEAREST, expand=True)\n",
        "\n",
        "    canvas = Image.new(\"L\", (height, height), 0)\n",
        "    if width < height:\n",
        "        left = (height - width) // 2\n",
        "        top = 0\n",
        "        canvas.paste(org_image_ro, (left, top))\n",
        "    else:\n",
        "        scale_factor = height / width\n",
        "        s_width = round(scale_factor * width)\n",
        "        s_height = round(scale_factor * height)\n",
        "        resized_image = org_image_ro.resize((s_width, s_height), Image.NEAREST)\n",
        "        left = (height - s_width) // 2\n",
        "        top = 0\n",
        "        canvas.paste(resized_image, (left, top))\n",
        "        del resized_image\n",
        "\n",
        "    del org_image_rs\n",
        "    del org_image_ro\n",
        "    del org_image\n",
        "\n",
        "    thumbnail = canvas.resize((target_width, target_height), Image.NEAREST)\n",
        "    return thumbnail\n",
        "\n",
        "\n",
        "def show_sequence(the_images):\n",
        "    l = len(the_images)\n",
        "    dim = math.ceil(math.sqrt(l))\n",
        "    plt.clf()\n",
        "    fig, axs = plt.subplots(dim, dim, figsize=(10, 10))\n",
        "    k = 0\n",
        "    for i in range(0, dim):\n",
        "        for j in range(0, dim):\n",
        "            img = the_images[k]\n",
        "            axs[i, j].imshow(img, cmap='gray')\n",
        "            axs[i, j].axis('off')\n",
        "            k += 1\n",
        "            if k == l:\n",
        "                plt.show()\n",
        "                return\n",
        "\n",
        "\n",
        "def show_triplet(triplets):\n",
        "    plt.clf()\n",
        "    fig, axs = plt.subplots(1, 3, figsize=(5, 5))\n",
        "\n",
        "    for k in range(0, 3):\n",
        "        img = triplets[k]\n",
        "        axs[k].imshow(img, cmap='gray')\n",
        "        axs[k].axis('off')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def pad_sequence(max_sequence_length, sequence, image_height, image_width, num_channels):\n",
        "    sequence = np.asarray(sequence)\n",
        "    padding_size = max_sequence_length - len(sequence)\n",
        "    if padding_size > 0:\n",
        "        padding_shape = (padding_size, image_height, image_width, num_channels)\n",
        "        padding_images = np.zeros(padding_shape)\n",
        "        padded_sequence = np.concatenate([sequence, padding_images], axis=0)\n",
        "    else:\n",
        "        padded_sequence = np.asarray(sequence)\n",
        "    return padded_sequence\n",
        "\n",
        "def pad_sequences(max_length, sequences, image_height, image_width, num_channels):\n",
        "    # Pad sequences to have the same length (pad with zero images)\n",
        "    padded_sequences = []\n",
        "    for sequence in sequences:\n",
        "        seq_len = len(sequence)\n",
        "        if seq_len == 0:\n",
        "            continue\n",
        "        if seq_len > max_length:\n",
        "            sequence = sequence[:max_length]\n",
        "            seq_len = len(sequence)\n",
        "\n",
        "        num_padding = max_length - seq_len\n",
        "        if num_padding > 0:\n",
        "            sequence = np.concatenate(\n",
        "                [sequence, np.zeros((num_padding, image_height, image_width, num_channels))])\n",
        "        padded_sequences.append(sequence)\n",
        "    return np.array(padded_sequences)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "helpers for training stats\n",
        "based on https://github.com/CrimyTheBold/tripletloss/blob/master/02%20-%20tripletloss%20MNIST.ipynb"
      ],
      "metadata": {
        "id": "GFOGAbKXGcDa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_probs(network,X,Y):\n",
        "    '''\n",
        "    Input\n",
        "        network : current NN to compute embeddings\n",
        "        X : tensor of shape (m,w,h,1) containing pics to evaluate\n",
        "        Y : tensor of shape (m,) containing true class\n",
        "\n",
        "    Returns\n",
        "        probs : array of shape (m,m) containing distances\n",
        "\n",
        "    '''\n",
        "    m = X.shape[0]\n",
        "    nbevaluation = int(m*(m-1)/2)\n",
        "    probs = np.zeros((nbevaluation))\n",
        "    y = np.zeros((nbevaluation))\n",
        "\n",
        "    #Compute all embeddings for all pics with current network\n",
        "    embeddings = network.predict(X)\n",
        "\n",
        "    size_embedding = embeddings.shape[1]\n",
        "\n",
        "    #For each pics of our dataset\n",
        "    k = 0\n",
        "    for i in range(m):\n",
        "            #Against all other images\n",
        "            for j in range(i+1,m):\n",
        "                #compute the probability of being the right decision : it should be 1 for right class, 0 for all other classes\n",
        "                probs[k] = -compute_dist(embeddings[i,:],embeddings[j,:])\n",
        "                if (Y[i]==Y[j]):\n",
        "                    y[k] = 1\n",
        "                    #print(\"{3}:{0} vs {1} : {2}\\tSAME\".format(i,j,probs[k],k))\n",
        "                else:\n",
        "                    y[k] = 0\n",
        "                    #print(\"{3}:{0} vs {1} : \\t\\t\\t{2}\\tDIFF\".format(i,j,probs[k],k))\n",
        "                k += 1\n",
        "    return probs,y\n",
        "#probs,yprobs = compute_probs(network,x_test_origin[:10,:,:,:],y_test_origin[:10])\n",
        "\n",
        "def compute_metrics(probs,yprobs):\n",
        "    '''\n",
        "    Returns\n",
        "        fpr : Increasing false positive rates such that element i is the false positive rate of predictions with score >= thresholds[i]\n",
        "        tpr : Increasing true positive rates such that element i is the true positive rate of predictions with score >= thresholds[i].\n",
        "        thresholds : Decreasing thresholds on the decision function used to compute fpr and tpr. thresholds[0] represents no instances being predicted and is arbitrarily set to max(y_score) + 1\n",
        "        auc : Area Under the ROC Curve metric\n",
        "    '''\n",
        "    # calculate AUC\n",
        "    auc = roc_auc_score(yprobs, probs)\n",
        "    # calculate roc curve\n",
        "    fpr, tpr, thresholds = roc_curve(yprobs, probs)\n",
        "\n",
        "    return fpr, tpr, thresholds,auc\n",
        "\n",
        "def compute_interdist(network):\n",
        "    '''\n",
        "    Computes sum of distances between all classes embeddings on our reference test image:\n",
        "        d(0,1) + d(0,2) + ... + d(0,9) + d(1,2) + d(1,3) + ... d(8,9)\n",
        "        A good model should have a large distance between all theses embeddings\n",
        "\n",
        "    Returns:\n",
        "        array of shape (nb_classes,nb_classes)\n",
        "    '''\n",
        "    res = np.zeros((nb_classes,nb_classes))\n",
        "\n",
        "    ref_images = np.zeros((nb_classes,img_rows,img_cols,1))\n",
        "\n",
        "    #generates embeddings for reference images\n",
        "    for i in range(nb_classes):\n",
        "        ref_images[i,:,:,:] = dataset_test[i][0,:,:,:]\n",
        "    ref_embeddings = network.predict(ref_images)\n",
        "\n",
        "    for i in range(nb_classes):\n",
        "        for j in range(nb_classes):\n",
        "            res[i,j] = compute_dist(ref_embeddings[i],ref_embeddings[j])\n",
        "    return res\n",
        "\n",
        "def draw_interdist(network,n_iteration):\n",
        "    interdist = compute_interdist(network)\n",
        "\n",
        "    data = []\n",
        "    for i in range(nb_classes):\n",
        "        data.append(np.delete(interdist[i,:],[i]))\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.set_title('Evaluating embeddings distance from each other after {0} iterations'.format(n_iteration))\n",
        "    ax.set_ylim([0,3])\n",
        "    plt.xlabel('Classes')\n",
        "    plt.ylabel('Distance')\n",
        "    ax.boxplot(data,showfliers=False,showbox=True)\n",
        "    locs, labels = plt.xticks()\n",
        "    plt.xticks(locs,np.arange(nb_classes))\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def find_nearest(array,value):\n",
        "    idx = np.searchsorted(array, value, side=\"left\")\n",
        "    if idx > 0 and (idx == len(array) or math.fabs(value - array[idx-1]) < math.fabs(value - array[idx])):\n",
        "        return array[idx-1],idx-1\n",
        "    else:\n",
        "        return array[idx],idx\n",
        "\n",
        "def draw_roc(fpr, tpr,thresholds):\n",
        "    #find threshold\n",
        "    targetfpr=1e-3\n",
        "    _, idx = find_nearest(fpr,targetfpr)\n",
        "    threshold = thresholds[idx]\n",
        "    recall = tpr[idx]\n",
        "\n",
        "\n",
        "    # plot no skill\n",
        "    plt.plot([0, 1], [0, 1], linestyle='--')\n",
        "    # plot the roc curve for the model\n",
        "    plt.plot(fpr, tpr, marker='.')\n",
        "    plt.title('AUC: {0:.3f}\\nSensitivity : {2:.1%} @FPR={1:.0e}\\nThreshold={3})'.format(auc,targetfpr,recall,abs(threshold) ))\n",
        "    # show the plot\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "zeQwS1E8Gaus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "J_XabRx8qb8X"
      },
      "outputs": [],
      "source": [
        "class FinalStopIteration(StopIteration):\n",
        "    def __init__(self, *args: object) -> None:\n",
        "        super().__init__(*args)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseLetterGenerator(Sequence):\n",
        "    def __init__(self, mode, user_ids, options: ModelOptions):\n",
        "        self.options = options\n",
        "        self.user_ids = [i for i in user_ids]\n",
        "        self.id_to_class = {user_id: i for i, user_id in enumerate(user_ids)}\n",
        "        self.input_shape = (options.image_height, options.image_width)\n",
        "        self.random_shuffle_amount = options.random_shuffle_amount\n",
        "        self.users_ds = {}\n",
        "        self.generators = {}\n",
        "        self.mode = mode\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.options.max_embedding_samples\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        self.generators = {}\n",
        "\n",
        "    def reset_generators(self):\n",
        "        self.generators = {}\n",
        "\n",
        "    def get_user_ds(self, user_id):\n",
        "        if user_id not in self.users_ds:\n",
        "            uds = UserDataset(user_id)\n",
        "            uds.warmup()\n",
        "            self.users_ds[user_id] = uds\n",
        "        return self.users_ds[user_id]\n",
        "\n",
        "    def get_letters_generator(self, user_id, is_anchor=False):\n",
        "        key = f\"anc{user_id}\" if is_anchor else str(user_id)\n",
        "        if key not in self.generators:\n",
        "            # print(f\"new generator for {user_id} anchor{is_anchor}\")\n",
        "            uds = self.get_user_ds(user_id)\n",
        "            self.generators[key] = uds.random_letters_generator(mode=self.mode, target_size=self.input_shape,\n",
        "                                                                original_only=is_anchor,\n",
        "                                                                random_shuffle_amount=self.random_shuffle_amount)\n",
        "        return self.generators[key]\n",
        "\n",
        "\n",
        "class LettersGenerator(BaseLetterGenerator):\n",
        "    def __init__(self, mode, user_ids, options: ModelOptions, total_users):\n",
        "        super().__init__(mode, user_ids, options)\n",
        "        self.total_users = total_users\n",
        "\n",
        "    def __len__(self):\n",
        "        lines = 20\n",
        "        #lines=10\n",
        "        users = len(self.user_ids)\n",
        "        letters = 50\n",
        "        #letters =30\n",
        "        random_shuffle_amount = self.options.random_shuffle_amount\n",
        "        types = len(ALLOWED_TYPES)\n",
        "\n",
        "        total_batches = (types * lines * users * letters * random_shuffle_amount) // self.options.batch_size\n",
        "        # print(f\"LettersGenerator __len__ {total_batches}\")\n",
        "        return total_batches\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        batch, labels = [], []\n",
        "        users_count = len(self.user_ids)\n",
        "        for s in range(self.options.batch_size):\n",
        "            user_id = random.choice(self.user_ids)\n",
        "            #print(f\"__getitem__{user_id}\")\n",
        "            letter, _, _, _ = next(self.get_letters_generator(user_id))\n",
        "            batch.append(letter)\n",
        "            labels.append(to_categorical(self.id_to_class[user_id], num_classes=self.total_users))\n",
        "            #print(labels)\n",
        "\n",
        "\n",
        "        if len(batch) == 0:\n",
        "            batch = np.zeros((self.options.batch_size, self.options.image_height, self.options.image_width, 1))\n",
        "            labels = np.zeros((self.options.batch_size,))\n",
        "        # print(f\"LettersGenerator batch: {len(batch)}\")\n",
        "        return np.asarray(batch), np.asarray(labels)\n",
        "\n",
        "\n",
        "class TripletsGenerator(BaseLetterGenerator):\n",
        "    def __init__(self, mode, user_ids, options: ModelOptions):\n",
        "        super().__init__(mode, user_ids, options)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        anchors, positives, negatives= [], [], []\n",
        "        for _ in range(self.options.batch_size):\n",
        "            positive_user, negative_user_id = random.sample(self.user_ids, 2)\n",
        "            #print(f\"TripletsGenerator __getitem__ {positive_user} {negative_user_id} \" )\n",
        "            for triplet in self.get_triplets(positive_user, negative_user_id):\n",
        "\n",
        "                if triplet is None:\n",
        "                    positive_user, negative_user_id = random.sample(self.user_ids, 2)\n",
        "                    continue\n",
        "                anchor, positive, negative = triplet\n",
        "                anchors.append(anchor)\n",
        "                positives.append(positive)\n",
        "                negatives.append(negative)\n",
        "\n",
        "        batch = [np.asarray(anchors), np.asarray(positives), np.asarray(negatives)]\n",
        "        return batch,[]\n",
        "\n",
        "    def get_triplets(self, positive_user, negative_user_id):\n",
        "        anc_letter, img_path, line_idx, split_index = next(self.get_letters_generator(positive_user, True))\n",
        "        uds = self.get_user_ds(positive_user)\n",
        "        filtered = list(filter(lambda im_type: im_type != img_path, ALLOWED_TYPES))\n",
        "        img_path = random.choice(filtered)\n",
        "        if random.randint(0,10) < 4:\n",
        "          positive_letter = uds.get_letter(img_path, line_idx, split_index, self.input_shape)\n",
        "        else:\n",
        "          positive_letter, _, _, _ = next(self.get_letters_generator(positive_user, False))\n",
        "        negative_letter, _, _, _ = next(self.get_letters_generator(negative_user_id, False))\n",
        "        if anc_letter is not None and positive_letter is not None and negative_letter is not None:\n",
        "            yield anc_letter, positive_letter, negative_letter\n"
      ],
      "metadata": {
        "id": "DAiKm6RfLShF"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display generators"
      ],
      "metadata": {
        "id": "uWuxItQd90E8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "user_ids = [11]\n",
        "num_classes = len(user_ids)\n",
        "\n",
        "num_epochs = 3\n",
        "\n",
        "model_options = ModelOptions(\n",
        "    num_classes=len(user_ids) ,\n",
        "    batch_size=100,\n",
        "    image_height=150,\n",
        "    image_width=150,\n",
        "    num_channels=1,\n",
        "    max_sequence_length=36,\n",
        "    random_shuffle_amount=1,\n",
        "    lstm_units=15\n",
        ")\n",
        "model_options"
      ],
      "metadata": {
        "id": "MiLhqwN09whD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f3b09f1-dbfa-4cbe-bbab-7f4a157514d3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'num_classes': 1, 'batch_size': 100, 'image_height': 150, 'image_width': 150, 'num_channels': 1, 'max_sequence_length': 36, 'random_shuffle_amount': 1, 'max_embedding_samples': 5, 'alpha': 0.2, 'embedding_dim': 512}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def display_triplets():\n",
        "\n",
        "  train_gen = TripletsGenerator(MODE_TRAIN, user_ids, model_options)\n",
        "  valid_gen = TripletsGenerator(MODE_VALIDATION, user_ids, model_options)\n",
        "\n",
        "  train_gen.on_epoch_end()\n",
        "  for i, batch_x in enumerate(train_gen):\n",
        "    anchors, possitives, negatives = batch_x[0]\n",
        "    for i in range(len(anchors)):\n",
        "        show_triplet([anchors[i], possitives[i],negatives[i]])\n",
        "\n",
        "    if i==5:\n",
        "      break\n",
        "    break\n",
        "    #for id, letter in enumerate(batch_x):\n",
        "    #  print(labels[id])\n",
        "    #  #image_dots(letter.squeeze()*255)\n",
        "    #  show_line(letter.squeeze())\n",
        "    #break\n",
        "\n",
        "\n",
        "    #break\n",
        "  train_gen.on_epoch_end()\n",
        "\n",
        "#full_data_set = DataSet()\n",
        "display_triplets()"
      ],
      "metadata": {
        "id": "-shJnAzGBs4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_letters():\n",
        "  train_gen = LettersGenerator(MODE_TRAIN, user_ids, model_options, len(user_ids))\n",
        "  valid_gen = LettersGenerator(MODE_VALIDATION, user_ids, model_options, len(user_ids))\n",
        "\n",
        "  train_gen.on_epoch_end()\n",
        "  for i, (batch_x, labels) in enumerate(train_gen):\n",
        "    #print(labels)\n",
        "    #pass\n",
        "    show_sequence(batch_x)\n",
        "    if i==5:\n",
        "      break\n",
        "    #for id, letter in enumerate(batch_x):\n",
        "    #  print(labels[id])\n",
        "    #  image_dots(letter.squeeze()*255)\n",
        "    #  if i==10:\n",
        "    #    break\n",
        "\n",
        "    #  show_line(letter.squeeze())\n",
        "    #break\n",
        "\n",
        "  train_gen.on_epoch_end()\n",
        "\n",
        "#full_data_set = DataSet()\n",
        "display_letters()"
      ],
      "metadata": {
        "id": "DtaJmW6dLmaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from numba import cuda\n",
        "device = cuda.get_current_device()\n",
        "device.reset()"
      ],
      "metadata": {
        "id": "D-kqxjvDL9pC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Incremental learning**"
      ],
      "metadata": {
        "id": "Mz_hvekSZM-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Basic classifiction - CNN up to 20 users"
      ],
      "metadata": {
        "id": "6vBINS9lEM4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimplifiedResnet:\n",
        "  def res_block(self, X, filters):\n",
        "    X_shortcut = X\n",
        "\n",
        "    # First component of main path\n",
        "    X = Conv2D(filters=filters, kernel_size=(3, 3), strides=(1, 1), padding='same')(X)\n",
        "    X = BatchNormalization()(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    # Second component of main path\n",
        "    X = Conv2D(filters=filters, kernel_size=(3, 3), strides=(1, 1), padding='same')(X)\n",
        "    X = BatchNormalization()(X)\n",
        "\n",
        "    # Add shortcut value to main path\n",
        "    X = Add()([X, X_shortcut])\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    return X\n",
        "\n",
        "  def get_model(self,num_classes, input_shape):\n",
        "      X_input = Input(input_shape)\n",
        "      X = Conv2D(64, (7, 7), strides=(2, 2), padding='same')(X_input)\n",
        "      X = BatchNormalization()(X)\n",
        "      X = Activation('relu')(X)\n",
        "      X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n",
        "\n",
        "      # Residual blocks\n",
        "      X = self.res_block(X, 64)\n",
        "      X = self.res_block(X, 64)\n",
        "\n",
        "      # Classifier\n",
        "      X = Flatten()(X)\n",
        "      X = Dense(num_classes, activation='softmax')(X)\n",
        "\n",
        "      # Create model\n",
        "      return  Model(inputs=X_input, outputs=X)"
      ],
      "metadata": {
        "id": "6dIZFEk4LCHw"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_classes= 20\n",
        "input_shape = (model_options.image_height, model_options.image_width, 1)\n",
        "model = SimplifiedResnet().get_model(num_classes=max_classes , input_shape=input_shape)\n",
        "\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "GIat0kFfKr0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model,num_epochs, train_gen, valid_gen,callbacks_list):\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "  model.fit(train_gen, epochs=num_epochs, batch_size=model_options.batch_size,\n",
        "            validation_data=valid_gen, verbose=1,callbacks=callbacks_list)\n",
        "  return model\n",
        "\n",
        "model_options = ModelOptions(\n",
        "    batch_size=100,\n",
        "    random_shuffle_amount=1,\n",
        ")\n",
        "\n",
        "filepath = MODEL_CHECKPOINT_PATH + \"incr_model_{epoch:02d}-{val_accuracy:.2f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1,  mode='max')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "\n",
        "max_classes= 20\n",
        "#num_epocs = 2\n",
        "num_epocs = 1\n",
        "input_shape = (model_options.image_height, model_options.image_width, 1)\n",
        "\n",
        "last_save_path = MODEL_CHECKPOINT_PATH + 'incr_model_12_users.h5'\n",
        "#last_save_path = None\n",
        "for i in range(13, max_classes):\n",
        "  model = SimplifiedResnet().get_model(num_classes=max_classes , input_shape=input_shape)\n",
        "  model.summary()\n",
        "  if last_save_path is not None:\n",
        "    print(f'loading weights from {last_save_path}')\n",
        "    old_model = load_model(last_save_path)\n",
        "    old_weights = old_model.get_weights()\n",
        "    model.set_weights(old_weights)\n",
        "    print(f'doe loading weights')\n",
        "\n",
        "  user_ids = [i for i in range(0, i)]\n",
        "  total_users =  len(user_ids)\n",
        "  print (f\"start traing on {total_users} users: ({user_ids}) using {num_epocs} epocs\")\n",
        "  train_gen = LettersGenerator(MODE_TRAIN, user_ids, model_options, max_classes)\n",
        "  valid_gen = LettersGenerator(MODE_VALIDATION, user_ids, model_options, max_classes)\n",
        "  model = train_model(model, num_epocs, train_gen, valid_gen,callbacks_list)\n",
        "  last_save_path = MODEL_CHECKPOINT_PATH + f'incr_model_{i}_users.h5'\n",
        "  print (f\"saving weights in {last_save_path}\")\n",
        "  model.save(last_save_path)\n",
        "  #num_epocs += 2\n",
        "\n"
      ],
      "metadata": {
        "id": "CHXcQp55Je1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "571a7b26-305d-4f2d-9451-036f9ab626b9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 150, 150, 1  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)              (None, 75, 75, 64)   3200        ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 75, 75, 64)  256         ['conv2d_5[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_5 (Activation)      (None, 75, 75, 64)   0           ['batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " max_pooling2d_1 (MaxPooling2D)  (None, 37, 37, 64)  0           ['activation_5[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)              (None, 37, 37, 64)   36928       ['max_pooling2d_1[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 37, 37, 64)  256         ['conv2d_6[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_6 (Activation)      (None, 37, 37, 64)   0           ['batch_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)              (None, 37, 37, 64)   36928       ['activation_6[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 37, 37, 64)  256         ['conv2d_7[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 37, 37, 64)   0           ['batch_normalization_7[0][0]',  \n",
            "                                                                  'max_pooling2d_1[0][0]']        \n",
            "                                                                                                  \n",
            " activation_7 (Activation)      (None, 37, 37, 64)   0           ['add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)              (None, 37, 37, 64)   36928       ['activation_7[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 37, 37, 64)  256         ['conv2d_8[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_8 (Activation)      (None, 37, 37, 64)   0           ['batch_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)              (None, 37, 37, 64)   36928       ['activation_8[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 37, 37, 64)  256         ['conv2d_9[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " add_3 (Add)                    (None, 37, 37, 64)   0           ['batch_normalization_9[0][0]',  \n",
            "                                                                  'activation_7[0][0]']           \n",
            "                                                                                                  \n",
            " activation_9 (Activation)      (None, 37, 37, 64)   0           ['add_3[0][0]']                  \n",
            "                                                                                                  \n",
            " flatten_1 (Flatten)            (None, 87616)        0           ['activation_9[0][0]']           \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 20)           1752340     ['flatten_1[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,904,532\n",
            "Trainable params: 1,903,892\n",
            "Non-trainable params: 640\n",
            "__________________________________________________________________________________________________\n",
            "loading weights from /gdrive/MyDrive/deep-learning/final-project/data-sets/model_checkpoints/incr_model_11_users.h5\n",
            "doe loading weights\n",
            "start traing on 12 users: ([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]) using 1 epocs\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/2_ImagesMedianBW/lines1_Page_11.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/0_Images/lines1_Page_11.jpgloading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/3_ImagesLinesRemovedBW/lines1_Page_11.jpg\n",
            "\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/1_ImagesRotated/lines1_Page_11.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/4_ImagesLinesRemoved/lines1_Page_11.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/0_Images/lines1_Page_05.jpgloading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/1_ImagesRotated/lines1_Page_05.jpg\n",
            "\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/2_ImagesMedianBW/lines1_Page_05.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/3_ImagesLinesRemovedBW/lines1_Page_05.jpgloading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/4_ImagesLinesRemoved/lines1_Page_05.jpg\n",
            "\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/2_ImagesMedianBW/lines1_Page_03.jpgloading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/1_ImagesRotated/lines1_Page_03.jpg\n",
            "\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/0_Images/lines1_Page_03.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/3_ImagesLinesRemovedBW/lines1_Page_03.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/4_ImagesLinesRemoved/lines1_Page_03.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/2_ImagesMedianBW/lines1_Page_10.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/1_ImagesRotated/lines1_Page_10.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/3_ImagesLinesRemovedBW/lines1_Page_10.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/0_Images/lines1_Page_10.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/4_ImagesLinesRemoved/lines1_Page_10.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/0_Images/lines1_Page_02.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/1_ImagesRotated/lines1_Page_02.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/2_ImagesMedianBW/lines1_Page_02.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/3_ImagesLinesRemovedBW/lines1_Page_02.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/4_ImagesLinesRemoved/lines1_Page_02.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/0_Images/lines1_Page_06.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/3_ImagesLinesRemovedBW/lines1_Page_06.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/1_ImagesRotated/lines1_Page_06.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/4_ImagesLinesRemoved/lines1_Page_06.jpgloading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/2_ImagesMedianBW/lines1_Page_06.jpg\n",
            "\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/0_Images/lines1_Page_01.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/1_ImagesRotated/lines1_Page_01.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/2_ImagesMedianBW/lines1_Page_01.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/3_ImagesLinesRemovedBW/lines1_Page_01.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/4_ImagesLinesRemoved/lines1_Page_01.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/0_Images/lines1_Page_12.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/2_ImagesMedianBW/lines1_Page_12.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/3_ImagesLinesRemovedBW/lines1_Page_12.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/1_ImagesRotated/lines1_Page_12.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/4_ImagesLinesRemoved/lines1_Page_12.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/1_ImagesRotated/lines1_Page_08.jpgloading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/2_ImagesMedianBW/lines1_Page_08.jpg\n",
            "\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/3_ImagesLinesRemovedBW/lines1_Page_08.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/0_Images/lines1_Page_08.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/4_ImagesLinesRemoved/lines1_Page_08.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/1_ImagesRotated/lines1_Page_09.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/2_ImagesMedianBW/lines1_Page_09.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/0_Images/lines1_Page_09.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/3_ImagesLinesRemovedBW/lines1_Page_09.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/4_ImagesLinesRemoved/lines1_Page_09.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/1_ImagesRotated/lines1_Page_07.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/0_Images/lines1_Page_07.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/2_ImagesMedianBW/lines1_Page_07.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/3_ImagesLinesRemovedBW/lines1_Page_07.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/4_ImagesLinesRemoved/lines1_Page_07.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/1_ImagesRotated/lines1_Page_04.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/0_Images/lines1_Page_04.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/2_ImagesMedianBW/lines1_Page_04.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/4_ImagesLinesRemoved/lines1_Page_04.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/3_ImagesLinesRemovedBW/lines1_Page_04.jpg\n",
            "180/180 [==============================] - ETA: 0s - loss: 1.1168 - accuracy: 0.6983\n",
            "Epoch 1: saving model to /gdrive/MyDrive/deep-learning/final-project/data-sets/model_checkpoints/incr_model_01-0.71.hdf5\n",
            "180/180 [==============================] - 1464s 8s/step - loss: 1.1168 - accuracy: 0.6983 - val_loss: 0.8735 - val_accuracy: 0.7057\n",
            "saving weights in /gdrive/MyDrive/deep-learning/final-project/data-sets/model_checkpoints/incr_model_12_users.h5\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 150, 150, 1  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)             (None, 75, 75, 64)   3200        ['input_3[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 75, 75, 64)  256         ['conv2d_10[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_10 (Activation)     (None, 75, 75, 64)   0           ['batch_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " max_pooling2d_2 (MaxPooling2D)  (None, 37, 37, 64)  0           ['activation_10[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)             (None, 37, 37, 64)   36928       ['max_pooling2d_2[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, 37, 37, 64)  256         ['conv2d_11[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_11 (Activation)     (None, 37, 37, 64)   0           ['batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)             (None, 37, 37, 64)   36928       ['activation_11[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 37, 37, 64)  256         ['conv2d_12[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_4 (Add)                    (None, 37, 37, 64)   0           ['batch_normalization_12[0][0]', \n",
            "                                                                  'max_pooling2d_2[0][0]']        \n",
            "                                                                                                  \n",
            " activation_12 (Activation)     (None, 37, 37, 64)   0           ['add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)             (None, 37, 37, 64)   36928       ['activation_12[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 37, 37, 64)  256         ['conv2d_13[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_13 (Activation)     (None, 37, 37, 64)   0           ['batch_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)             (None, 37, 37, 64)   36928       ['activation_13[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 37, 37, 64)  256         ['conv2d_14[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_5 (Add)                    (None, 37, 37, 64)   0           ['batch_normalization_14[0][0]', \n",
            "                                                                  'activation_12[0][0]']          \n",
            "                                                                                                  \n",
            " activation_14 (Activation)     (None, 37, 37, 64)   0           ['add_5[0][0]']                  \n",
            "                                                                                                  \n",
            " flatten_2 (Flatten)            (None, 87616)        0           ['activation_14[0][0]']          \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 20)           1752340     ['flatten_2[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,904,532\n",
            "Trainable params: 1,903,892\n",
            "Non-trainable params: 640\n",
            "__________________________________________________________________________________________________\n",
            "loading weights from /gdrive/MyDrive/deep-learning/final-project/data-sets/model_checkpoints/incr_model_12_users.h5\n",
            "doe loading weights\n",
            "start traing on 13 users: ([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]) using 1 epocs\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/4_ImagesLinesRemoved/lines1_Page_13.jpgloading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/2_ImagesMedianBW/lines1_Page_13.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/1_ImagesRotated/lines1_Page_13.jpgloading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/3_ImagesLinesRemovedBW/lines1_Page_13.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/0_Images/lines1_Page_13.jpg\n",
            "\n",
            "\n",
            "  7/195 [>.............................] - ETA: 19:25 - loss: 1.7828 - accuracy: 0.6414"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-cec64a4a0268>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0mtrain_gen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLettersGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODE_TRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m   \u001b[0mvalid_gen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLettersGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODE_VALIDATION\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m   \u001b[0mlast_save_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMODEL_CHECKPOINT_PATH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34mf'incr_model_{i}_users.h5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m   \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34mf\"saving weights in {last_save_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-cec64a4a0268>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, num_epochs, train_gen, valid_gen, callbacks_list)\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   model.fit(train_gen, epochs=num_epochs, batch_size=model_options.batch_size,\n\u001b[0m\u001b[1;32m      5\u001b[0m             validation_data=valid_gen, verbose=1,callbacks=callbacks_list)\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1683\u001b[0m                         ):\n\u001b[1;32m   1684\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1685\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1686\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    924\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m       (concrete_function,\n\u001b[1;32m    142\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    144\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1755\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1756\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1758\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1759\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    382\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model for generating embedding vector (512) for user letters image**"
      ],
      "metadata": {
        "id": "XG4qRYh0EVkK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EmbeddingModel:\n",
        "    def get_model(self, saved_path, input_shape, embedding_dim):\n",
        "\n",
        "        cnn_networkd = load_cnn_network(saved_path, input_shape)\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(cnn_networkd)\n",
        "        model.add(Conv2D(64, (1, 1), activation='relu'))\n",
        "        model.add(GlobalAveragePooling2D())\n",
        "        model.add(Dense(embedding_dim, activation='relu', kernel_regularizer=l2(1e-3), kernel_initializer='he_uniform'))\n",
        "        model.add(Lambda(lambda x: K.l2_normalize(x, axis=1))) # L2 normalization layer for embedding\n",
        "        return model\n",
        "\n",
        "def load_cnn_network(saved_path, input_shape):\n",
        "  m = load_model(saved_path)\n",
        "  model = Model(m.inputs, m.layers[-3].output)\n",
        "  model.summary()\n",
        "  for layer in model.layers:\n",
        "    layer.trainable = False\n",
        "  return model\n"
      ],
      "metadata": {
        "id": "XCsnj_F8xis9"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_options = ModelOptions(\n",
        "    batch_size=100,\n",
        "    random_shuffle_amount=1,\n",
        ")\n",
        "input_shape = (model_options.image_height, model_options.image_width, 1)\n",
        "last_save_path = MODEL_CHECKPOINT_PATH + 'incr_model_12_users.h5'\n",
        "#cnn_network = load_cnn_network(last_save_path,input_shape)\n",
        "#cnn_network.summary()\n",
        "embedding = EmbeddingModel().get_model(last_save_path, input_shape, model_options.embedding_dim)\n",
        "embedding.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_DoFk8M2kFp",
        "outputId": "c1e4aa48-1769-4a7a-96af-9ae4ec26dee4"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 150, 150, 1  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)              (None, 75, 75, 64)   3200        ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 75, 75, 64)  256         ['conv2d_5[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_5 (Activation)      (None, 75, 75, 64)   0           ['batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " max_pooling2d_1 (MaxPooling2D)  (None, 37, 37, 64)  0           ['activation_5[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)              (None, 37, 37, 64)   36928       ['max_pooling2d_1[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 37, 37, 64)  256         ['conv2d_6[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_6 (Activation)      (None, 37, 37, 64)   0           ['batch_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)              (None, 37, 37, 64)   36928       ['activation_6[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 37, 37, 64)  256         ['conv2d_7[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 37, 37, 64)   0           ['batch_normalization_7[0][0]',  \n",
            "                                                                  'max_pooling2d_1[0][0]']        \n",
            "                                                                                                  \n",
            " activation_7 (Activation)      (None, 37, 37, 64)   0           ['add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)              (None, 37, 37, 64)   36928       ['activation_7[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 37, 37, 64)  256         ['conv2d_8[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_8 (Activation)      (None, 37, 37, 64)   0           ['batch_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)              (None, 37, 37, 64)   36928       ['activation_8[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 37, 37, 64)  256         ['conv2d_9[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " add_3 (Add)                    (None, 37, 37, 64)   0           ['batch_normalization_9[0][0]',  \n",
            "                                                                  'activation_7[0][0]']           \n",
            "                                                                                                  \n",
            " activation_9 (Activation)      (None, 37, 37, 64)   0           ['add_3[0][0]']                  \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 152,192\n",
            "Trainable params: 151,552\n",
            "Non-trainable params: 640\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " model (Functional)          (None, 37, 37, 64)        152192    \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 37, 37, 64)        4160      \n",
            "                                                                 \n",
            " global_average_pooling2d (G  (None, 64)               0         \n",
            " lobalAveragePooling2D)                                          \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               33280     \n",
            "                                                                 \n",
            " lambda (Lambda)             (None, 512)               0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 189,632\n",
            "Trainable params: 37,440\n",
            "Non-trainable params: 152,192\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SiameseModel:\n",
        "    def __init__(self, options: ModelOptions, embedding):\n",
        "        super().__init__()\n",
        "        self.options = options\n",
        "        self.alpha = options.alpha\n",
        "        input_shape = (model_options.image_height,model_options.image_width ,1)\n",
        "        self.embedding = embedding\n",
        "\n",
        "    def get_model(self):\n",
        "      #triplet_input = Input(shape=(self.options.image_height, self.options.image_width, 1), name='triplet_input')\n",
        "\n",
        "      input_shape = (model_options.image_height,model_options.image_width ,1)\n",
        "      anchor_input = Input(input_shape, name=\"anchor_input\")\n",
        "      positive_input = Input(input_shape, name=\"positive_input\")\n",
        "      negative_input = Input(input_shape, name=\"negative_input\")\n",
        "\n",
        "      #anchor_input = Lambda(lambda x: x[0])(triplet_input)\n",
        "      #positive_input = Lambda(lambda x: x[1])(triplet_input)\n",
        "      #negative_input = Lambda(lambda x: x[2])(triplet_input)\n",
        "\n",
        "      enc_anchor = self.embedding(anchor_input)\n",
        "      enc_positive = self.embedding(positive_input)\n",
        "      enc_negative = self.embedding(negative_input)\n",
        "\n",
        "      loss_layer = TripletLossLayer(alpha=self.alpha, name='triplet_loss_layer')([enc_anchor, enc_positive, enc_negative])\n",
        "\n",
        "      model = Model(inputs=[anchor_input,positive_input,negative_input],outputs= loss_layer)\n",
        "      return model\n",
        "    def get_embedding(self):\n",
        "      return self.embedding\n",
        "\n",
        "class TripletLossLayer(Layer):\n",
        "    def __init__(self, alpha, **kwargs):\n",
        "        self.alpha = alpha\n",
        "        super(TripletLossLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def triplet_loss(self, inputs):\n",
        "        anchor, positive, negative = inputs\n",
        "        p_dist = K.sum(K.square(anchor-positive), axis=-1)\n",
        "        n_dist = K.sum(K.square(anchor-negative), axis=-1)\n",
        "        return K.sum(K.maximum(p_dist - n_dist + self.alpha, 0), axis=0)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        loss = self.triplet_loss(inputs)\n",
        "        self.add_loss(loss)\n",
        "        return loss\n",
        "\n"
      ],
      "metadata": {
        "id": "4rpEbxoFvZX6"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_data_set = DataSet()\n",
        "filepath = MODEL_CHECKPOINT_PATH + \"siamese-model-{epoch:02d}-{loss:.2f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "num_epochs=20\n",
        "user_ids =[i for i in range(0,10)]\n",
        "#user_ids = [random.randint(1,40),random.randint(41,80)]\n",
        "model_options = ModelOptions(\n",
        "    num_classes=len(user_ids),\n",
        "    batch_size=100,\n",
        "    random_shuffle_amount=1,\n",
        "    alpha = 0.2,\n",
        "    embedding_dim=512\n",
        ")\n",
        "\n",
        "num_classes = model_options.num_classes\n",
        "train_gen = TripletsGenerator(MODE_TRAIN, user_ids, model_options)\n",
        "valid_gen = TripletsGenerator(MODE_VALIDATION, user_ids, model_options)\n",
        "\n",
        "sm_network = SiameseModel(model_options, embedding)\n",
        "model = sm_network.get_model()\n",
        "opt =  Adam(learning_rate=1e-3)\n",
        "model.compile(optimizer=opt, loss=None)\n",
        "model.summary()\n",
        "\n",
        "plot_model(model,show_shapes=True, show_layer_names=True)\n",
        "history = model.fit(train_gen, epochs=num_epochs,batch_size=model_options.batch_size,\n",
        "#                   validation_data=valid_gen, verbose=1)\n",
        "                    verbose=1, callbacks=callbacks_list)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4dzVvL3v1rA",
        "outputId": "297e7460-5cd9-4c39-fd20-69871b392032"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " anchor_input (InputLayer)      [(None, 150, 150, 1  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " positive_input (InputLayer)    [(None, 150, 150, 1  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " negative_input (InputLayer)    [(None, 150, 150, 1  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " sequential (Sequential)        (None, 512)          189632      ['anchor_input[0][0]',           \n",
            "                                                                  'positive_input[0][0]',         \n",
            "                                                                  'negative_input[0][0]']         \n",
            "                                                                                                  \n",
            " triplet_loss_layer (TripletLos  ()                  0           ['sequential[0][0]',             \n",
            " sLayer)                                                          'sequential[1][0]',             \n",
            "                                                                  'sequential[2][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 189,632\n",
            "Trainable params: 37,440\n",
            "Non-trainable params: 152,192\n",
            "__________________________________________________________________________________________________\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/0_Images/lines1_Page_06.jpgloading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/3_ImagesLinesRemovedBW/lines1_Page_06.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/1_ImagesRotated/lines1_Page_06.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/2_ImagesMedianBW/lines1_Page_06.jpg\n",
            "\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/4_ImagesLinesRemoved/lines1_Page_06.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/0_Images/lines1_Page_07.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/1_ImagesRotated/lines1_Page_07.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/4_ImagesLinesRemoved/lines1_Page_07.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/2_ImagesMedianBW/lines1_Page_07.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/3_ImagesLinesRemovedBW/lines1_Page_07.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/2_ImagesMedianBW/lines1_Page_09.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/0_Images/lines1_Page_09.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/1_ImagesRotated/lines1_Page_09.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/3_ImagesLinesRemovedBW/lines1_Page_09.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/4_ImagesLinesRemoved/lines1_Page_09.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/1_ImagesRotated/lines1_Page_08.jpgloading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/2_ImagesMedianBW/lines1_Page_08.jpg\n",
            "\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/0_Images/lines1_Page_08.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/3_ImagesLinesRemovedBW/lines1_Page_08.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/4_ImagesLinesRemoved/lines1_Page_08.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/1_ImagesRotated/lines1_Page_05.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/0_Images/lines1_Page_05.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/4_ImagesLinesRemoved/lines1_Page_05.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/3_ImagesLinesRemovedBW/lines1_Page_05.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/2_ImagesMedianBW/lines1_Page_05.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/0_Images/lines1_Page_04.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/3_ImagesLinesRemovedBW/lines1_Page_04.jpgloading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/4_ImagesLinesRemoved/lines1_Page_04.jpg\n",
            "\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/1_ImagesRotated/lines1_Page_04.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/2_ImagesMedianBW/lines1_Page_04.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/0_Images/lines1_Page_10.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/1_ImagesRotated/lines1_Page_10.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/3_ImagesLinesRemovedBW/lines1_Page_10.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/2_ImagesMedianBW/lines1_Page_10.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/4_ImagesLinesRemoved/lines1_Page_10.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/0_Images/lines1_Page_02.jpgloading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/1_ImagesRotated/lines1_Page_02.jpg\n",
            "\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/2_ImagesMedianBW/lines1_Page_02.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/3_ImagesLinesRemovedBW/lines1_Page_02.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/4_ImagesLinesRemoved/lines1_Page_02.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/2_ImagesMedianBW/lines1_Page_03.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/1_ImagesRotated/lines1_Page_03.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/3_ImagesLinesRemovedBW/lines1_Page_03.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/0_Images/lines1_Page_03.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/4_ImagesLinesRemoved/lines1_Page_03.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/1_ImagesRotated/lines1_Page_01.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/2_ImagesMedianBW/lines1_Page_01.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/3_ImagesLinesRemovedBW/lines1_Page_01.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/0_Images/lines1_Page_01.jpg\n",
            "loading image from /gdrive/MyDrive/deep-learning/final-project/data-sets/4_ImagesLinesRemoved/lines1_Page_01.jpg\n",
            "Epoch 1/20\n",
            "5/5 [==============================] - ETA: 0s - loss: 14.0291\n",
            "Epoch 1: loss improved from inf to 14.02905, saving model to /gdrive/MyDrive/deep-learning/final-project/data-sets/model_checkpoints/siamese-model-01-14.03.hdf5\n",
            "5/5 [==============================] - 34s 6s/step - loss: 14.0291\n",
            "Epoch 2/20\n",
            "5/5 [==============================] - ETA: 0s - loss: 13.1391\n",
            "Epoch 2: loss improved from 14.02905 to 13.13908, saving model to /gdrive/MyDrive/deep-learning/final-project/data-sets/model_checkpoints/siamese-model-02-13.14.hdf5\n",
            "5/5 [==============================] - 28s 5s/step - loss: 13.1391\n",
            "Epoch 3/20\n",
            "5/5 [==============================] - ETA: 0s - loss: 10.5979\n",
            "Epoch 3: loss improved from 13.13908 to 10.59795, saving model to /gdrive/MyDrive/deep-learning/final-project/data-sets/model_checkpoints/siamese-model-03-10.60.hdf5\n",
            "5/5 [==============================] - 29s 5s/step - loss: 10.5979\n",
            "Epoch 4/20\n",
            "5/5 [==============================] - ETA: 0s - loss: 9.7058\n",
            "Epoch 4: loss improved from 10.59795 to 9.70575, saving model to /gdrive/MyDrive/deep-learning/final-project/data-sets/model_checkpoints/siamese-model-04-9.71.hdf5\n",
            "5/5 [==============================] - 29s 5s/step - loss: 9.7058\n",
            "Epoch 5/20\n",
            "5/5 [==============================] - ETA: 0s - loss: 9.2316\n",
            "Epoch 5: loss improved from 9.70575 to 9.23158, saving model to /gdrive/MyDrive/deep-learning/final-project/data-sets/model_checkpoints/siamese-model-05-9.23.hdf5\n",
            "5/5 [==============================] - 29s 6s/step - loss: 9.2316\n",
            "Epoch 6/20\n",
            "5/5 [==============================] - ETA: 0s - loss: 8.2562\n",
            "Epoch 6: loss improved from 9.23158 to 8.25623, saving model to /gdrive/MyDrive/deep-learning/final-project/data-sets/model_checkpoints/siamese-model-06-8.26.hdf5\n",
            "5/5 [==============================] - 29s 6s/step - loss: 8.2562\n",
            "Epoch 7/20\n",
            "5/5 [==============================] - ETA: 0s - loss: 7.9027\n",
            "Epoch 7: loss improved from 8.25623 to 7.90267, saving model to /gdrive/MyDrive/deep-learning/final-project/data-sets/model_checkpoints/siamese-model-07-7.90.hdf5\n",
            "5/5 [==============================] - 31s 6s/step - loss: 7.9027\n",
            "Epoch 8/20\n",
            "5/5 [==============================] - ETA: 0s - loss: 8.5927\n",
            "Epoch 8: loss did not improve from 7.90267\n",
            "5/5 [==============================] - 27s 5s/step - loss: 8.5927\n",
            "Epoch 9/20\n",
            "5/5 [==============================] - ETA: 0s - loss: 8.3221\n",
            "Epoch 9: loss did not improve from 7.90267\n",
            "5/5 [==============================] - 29s 6s/step - loss: 8.3221\n",
            "Epoch 10/20\n",
            "5/5 [==============================] - ETA: 0s - loss: 7.1229\n",
            "Epoch 10: loss improved from 7.90267 to 7.12288, saving model to /gdrive/MyDrive/deep-learning/final-project/data-sets/model_checkpoints/siamese-model-10-7.12.hdf5\n",
            "5/5 [==============================] - 28s 5s/step - loss: 7.1229\n",
            "Epoch 11/20\n",
            "5/5 [==============================] - ETA: 0s - loss: 7.4585\n",
            "Epoch 11: loss did not improve from 7.12288\n",
            "5/5 [==============================] - 28s 5s/step - loss: 7.4585\n",
            "Epoch 12/20\n",
            "5/5 [==============================] - ETA: 0s - loss: 6.9527\n",
            "Epoch 12: loss improved from 7.12288 to 6.95272, saving model to /gdrive/MyDrive/deep-learning/final-project/data-sets/model_checkpoints/siamese-model-12-6.95.hdf5\n",
            "5/5 [==============================] - 29s 6s/step - loss: 6.9527\n",
            "Epoch 13/20\n",
            "5/5 [==============================] - ETA: 0s - loss: 7.1986\n",
            "Epoch 13: loss did not improve from 6.95272\n",
            "5/5 [==============================] - 29s 6s/step - loss: 7.1986\n",
            "Epoch 14/20\n",
            "5/5 [==============================] - ETA: 0s - loss: 8.3971\n",
            "Epoch 14: loss did not improve from 6.95272\n",
            "5/5 [==============================] - 27s 5s/step - loss: 8.3971\n",
            "Epoch 15/20\n",
            "5/5 [==============================] - ETA: 0s - loss: 6.9626\n",
            "Epoch 15: loss did not improve from 6.95272\n",
            "5/5 [==============================] - 27s 5s/step - loss: 6.9626\n",
            "Epoch 16/20\n",
            "5/5 [==============================] - ETA: 0s - loss: 7.9542\n",
            "Epoch 16: loss did not improve from 6.95272\n",
            "5/5 [==============================] - 27s 5s/step - loss: 7.9542\n",
            "Epoch 17/20\n",
            "5/5 [==============================] - ETA: 0s - loss: 7.8302\n",
            "Epoch 17: loss did not improve from 6.95272\n",
            "5/5 [==============================] - 28s 5s/step - loss: 7.8302\n",
            "Epoch 18/20\n",
            "5/5 [==============================] - ETA: 0s - loss: 7.2642\n",
            "Epoch 18: loss did not improve from 6.95272\n",
            "5/5 [==============================] - 29s 6s/step - loss: 7.2642\n",
            "Epoch 19/20\n",
            "5/5 [==============================] - ETA: 0s - loss: 6.9311\n",
            "Epoch 19: loss improved from 6.95272 to 6.93111, saving model to /gdrive/MyDrive/deep-learning/final-project/data-sets/model_checkpoints/siamese-model-19-6.93.hdf5\n",
            "5/5 [==============================] - 29s 6s/step - loss: 6.9311\n",
            "Epoch 20/20\n",
            "5/5 [==============================] - ETA: 0s - loss: 7.0122\n",
            "Epoch 20: loss did not improve from 6.93111\n",
            "5/5 [==============================] - 28s 6s/step - loss: 7.0122\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "continue training"
      ],
      "metadata": {
        "id": "ZAxW_PYU9BCQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "full_data_set = DataSet()\n",
        "\n",
        "\n",
        "num_epochs=20\n",
        "user_ids =[i for i in range(1,12)]\n",
        "\n",
        "\n",
        "train_gen = TripletsGenerator(MODE_TRAIN, user_ids, model_options)\n",
        "valid_gen = TripletsGenerator(MODE_VALIDATION, user_ids, model_options)\n",
        "\n",
        "\n",
        "\n",
        "model.load_weights(MODEL_CHECKPOINT_PATH+'siamese-model-20-6.59.hdf5')\n",
        "history = model.fit(train_gen, epochs=num_epochs,batch_size=model_options.batch_size,\n",
        "#                   validation_data=valid_gen, verbose=1)\n",
        "                    verbose=1, callbacks=callbacks_list)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "PFdYvcYW8qfR",
        "outputId": "19eb9c49-d3b2-42cd-9fce-fda65d8ede3f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f112c4abb26e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfull_data_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0muser_ids\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'DataSet' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding.get_weights())\n"
      ],
      "metadata": {
        "id": "r9YqM9jQoidX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filepath =MODEL_CHECKPOINT_PATH + \"siamese-embedding.h5\"\n",
        "embedding.save_weights(filepath=filepath, overwrite=True)"
      ],
      "metadata": {
        "id": "IGFxCTGZPfiM"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLWrRRQ7uQqV",
        "outputId": "c84b1d25-2399-49e9-d96c-7bbbc0b74d55"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.engine.sequential.Sequential at 0x78b4e96f8040>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "featured_img = embedding.predict(np.ones((1,150,150,1)))\n",
        "print(featured_img)\n"
      ],
      "metadata": {
        "id": "-tAMh-9jy04U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **EMBEDDING + LSTM + ATTENTION Classfication**"
      ],
      "metadata": {
        "id": "Tm6Ht88ZH2Ko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SequenceGenerator(Sequence):\n",
        "\n",
        "    def __init__(self, mode, user_ids, model_options: ModelOptions):\n",
        "        self.options = model_options\n",
        "        self.user_ids = [i for i in user_ids]\n",
        "        self.id_to_class = {user_id: i for i, user_id in enumerate(user_ids)}\n",
        "        self.input_shape = (model_options.image_height, model_options.image_width)\n",
        "        self.usage_stats = {}\n",
        "        self.num_classes = model_options.num_classes\n",
        "        self.users_ds = {}\n",
        "        self.generators = {}\n",
        "        self.mode = mode\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        self.generators = {}\n",
        "\n",
        "    def incr_usage(self, user_id):\n",
        "        if user_id not in self.usage_stats:\n",
        "            self.usage_stats[user_id] = 0\n",
        "        self.usage_stats[user_id] += 1\n",
        "\n",
        "    def max_usages_per_user(self):\n",
        "        lines = 50\n",
        "        return lines * self.options.random_shuffle_amount\n",
        "\n",
        "    def get_user_ds(self, user_id):\n",
        "        if user_id not in self.users_ds:\n",
        "            uds = UserDataset(user_id)\n",
        "            uds.warmup()\n",
        "            self.users_ds[user_id] = uds\n",
        "        self.incr_usage(user_id)\n",
        "        return self.users_ds[user_id]\n",
        "\n",
        "    def get_sequence_generator(self, user_id):\n",
        "        if user_id not in self.generators:\n",
        "            #print(f\"new generator for {user_id}\")\n",
        "            uds = self.get_user_ds(user_id)\n",
        "            self.generators[user_id] = uds.random_line_generator(mode=self.mode,\n",
        "                                                                 max_sequence_length=self.options.max_sequence_length,\n",
        "                                                                 target_size=self.input_shape)\n",
        "        return self.generators[user_id]\n",
        "\n",
        "    def __len__(self):\n",
        "        lines = 20\n",
        "        users = len(self.user_ids)\n",
        "        random_shuffle_amount = self.options.random_shuffle_amount\n",
        "        types = len(ALLOWED_TYPES)\n",
        "\n",
        "        total_batches = (types * lines * users * random_shuffle_amount) // self.options.batch_size\n",
        "        return total_batches\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        batch, labels = [], []\n",
        "        for s in range(self.options.batch_size):\n",
        "            user_id = random.choice(self.user_ids)\n",
        "            sequence = next(self.get_sequence_generator(user_id))\n",
        "            batch.append(sequence)\n",
        "            labels.append(to_categorical(self.id_to_class[user_id], num_classes=self.options.num_classes))\n",
        "\n",
        "        return np.asarray(batch), np.asarray(labels)"
      ],
      "metadata": {
        "id": "qnwm-pHAIUM3"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import LSTM, Dense, TimeDistributed, Input, AdditiveAttention, Flatten\n",
        "\n",
        "class CnnLstmAttentionModel:\n",
        "    def __init__(self, options: ModelOptions, embedding_model):\n",
        "        self.model_options = options\n",
        "        self.embedding_model = embedding_model\n",
        "\n",
        "\n",
        "\n",
        "    def get_model(self, options):\n",
        "        input_layer = Input(shape=(self.model_options.max_sequence_length, self.model_options.image_height,\n",
        "                                   self.model_options.image_width, 1))\n",
        "\n",
        "        # 2) Use the CustomCNN as the embedding\n",
        "        self.embedding_model.trainable = False\n",
        "\n",
        "        # Apply CustomCNN to each image in the sequence\n",
        "        sequence_embedding = TimeDistributed(self.embedding_model)(input_layer)\n",
        "\n",
        "        # Flatten the output of the CustomCNN model\n",
        "        flattened_sequence = TimeDistributed(tf.keras.layers.Flatten())(sequence_embedding)\n",
        "\n",
        "        # LSTM Layer\n",
        "        lstm_output = LSTM(options['lstm_units'])(flattened_sequence)\n",
        "\n",
        "        # Additive Attention Layer\n",
        "        attention_output = AdditiveAttention()([lstm_output, lstm_output])\n",
        "\n",
        "        # Flattening the output for the Dense layer\n",
        "        attention_output_flat = Flatten()(attention_output)\n",
        "\n",
        "        dense_output = Dense(options['dense_units'], activation='relu')(attention_output_flat)\n",
        "\n",
        "        # 5) Classification Layer\n",
        "        output = Dense(options['num_classes'], activation='softmax')(dense_output)\n",
        "\n",
        "        # Build the model\n",
        "        model = Model(inputs=input_layer, outputs=output)\n",
        "\n",
        "        return model\n",
        "\n",
        "    def _convert_to_embedding_model(self, base_model, layer_name):\n",
        "        return Model(base_model.inputs, base_model.get_layer(layer_name).output)"
      ],
      "metadata": {
        "id": "-VcwTXU5H1tH"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "filepath = MODEL_CHECKPOINT_PATH + \"cnnlstm-{epoch:02d}-{val_accuracy:.2f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "num_epochs = 10\n",
        "user_ids = [i for i in range(1, 10)]\n",
        "\n",
        "model_options = ModelOptions(\n",
        "    num_classes=len(user_ids),\n",
        "    batch_size=50,\n",
        "    image_height=150,\n",
        "    image_width=150,\n",
        "    num_channels=1,\n",
        "    max_sequence_length=50,\n",
        "    random_shuffle_amount=32\n",
        ")\n",
        "\n",
        "layers_options = {\n",
        "    'lstm_units': 10,\n",
        "    'dense_units': 4,\n",
        "    'num_classes': model_options.num_classes\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "\n",
        "# If a GPU is available, the TensorFlow should default to it\n",
        "if tf.test.gpu_device_name():\n",
        "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
        "else:\n",
        "    print(\"Please install GPU version of TF\")\n",
        "\n",
        "input_shape = (model_options.image_height, model_options.image_width, 1)\n",
        "num_classes = len(user_ids)\n",
        "train_gen = SequenceGenerator(MODE_TRAIN, user_ids, model_options)\n",
        "valid_gen = SequenceGenerator(MODE_VALIDATION, user_ids, model_options)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "EbuhSNMFIHIT",
        "outputId": "c5ba5475-b80a-4d56-ef6f-21e7aa019937"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-8c1f453f9501>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMODEL_CHECKPOINT_PATH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"cnnlstm-{epoch:02d}-{val_accuracy:.2f}.hdf5\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'max'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mcallbacks_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'MODEL_CHECKPOINT_PATH' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sm = CnnLstmAttentionModel(model_options,embedding)\n",
        "model = sm.get_model(layers_options)\n",
        "\n",
        "opt =  Adam(learning_rate=1e-6)\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "history = model.fit(train_gen, epochs=num_epochs, batch_size=model_options.batch_size,\n",
        "                    validation_data=valid_gen, verbose=1, callbacks=callbacks_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2q5x0YYNpod",
        "outputId": "080f2dbf-cb17-43aa-e541-6ef3610dcf57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 50, 150, 15  0           []                               \n",
            "                                0, 1)]                                                            \n",
            "                                                                                                  \n",
            " time_distributed_2 (TimeDistri  (None, 50, 512)     189632      ['input_2[0][0]']                \n",
            " buted)                                                                                           \n",
            "                                                                                                  \n",
            " time_distributed_3 (TimeDistri  (None, 50, 512)     0           ['time_distributed_2[0][0]']     \n",
            " buted)                                                                                           \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)                  (None, 128)          328192      ['time_distributed_3[0][0]']     \n",
            "                                                                                                  \n",
            " additive_attention_1 (Additive  (None, 128)         128         ['lstm_1[0][0]',                 \n",
            " Attention)                                                       'lstm_1[0][0]']                 \n",
            "                                                                                                  \n",
            " flatten_3 (Flatten)            (None, 128)          0           ['additive_attention_1[0][0]']   \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 64)           8256        ['flatten_3[0][0]']              \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 9)            585         ['dense_3[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 526,793\n",
            "Trainable params: 337,161\n",
            "Non-trainable params: 189,632\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "sS-WEVQjPCTo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN NETWORK - from per image classification - DON'T USE IT"
      ],
      "metadata": {
        "id": "a-MQdbPSHgZy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Activation\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import Input\n",
        "\n",
        "\n",
        "class CustomCNN:\n",
        "    def __init__(self, input_shape):\n",
        "        self.input_shape = input_shape\n",
        "\n",
        "    def get_model(self, options):\n",
        "        model = Sequential()\n",
        "        model.add(Input(shape=self.input_shape))\n",
        "\n",
        "        # Adding Convolutional Layers\n",
        "        for i in range(options['depth']):\n",
        "            print(f\"model.add(Conv2D(filters={options['filters'][i]}, kernel_size={options['kernel_sizes'][i]},strides={options['strides'][i]},padding={options['padding'][i]}))\")\n",
        "            model.add(Conv2D(filters=options['filters'][i],\n",
        "                             kernel_size=options['kernel_sizes'][i],\n",
        "                             strides=options['strides'][i],\n",
        "                             padding=options['padding'][i]))\n",
        "            print(f\"model.add(Activation({options['conv_activation']}))\")\n",
        "            model.add(Activation(options['conv_activation']))\n",
        "            if options['pooling'][i]:\n",
        "                print(f\"model.add(MaxPooling2D(pool_size={options['pool_sizes'][i]}, strides={options['pool_strides'][i]}))\")\n",
        "                model.add(MaxPooling2D(pool_size=options['pool_sizes'][i],\n",
        "                                       strides=options['pool_strides'][i]))\n",
        "        print(f\"Flatten()\")\n",
        "        model.add(Flatten())  # Flattening the 2D arrays for fully connected layers\n",
        "\n",
        "        # Adding Fully Connected Layers\n",
        "        for i in range(options['fc_layers']):\n",
        "            print(f\"model.add(Dense({options['fc_units'][i]}))\")\n",
        "            model.add(Dense(options['fc_units'][i]))\n",
        "            print(f\"model.add(Activation({options['fc_activation']}))\")\n",
        "            model.add(Activation(options['fc_activation']))\n",
        "            print(f\"model.add(Dropout({options['dropout_rate']}))\")\n",
        "            model.add(Dropout(options['dropout_rate']))\n",
        "\n",
        "        # Output Layer\n",
        "        print(f\"model.add(Dense({options['num_classes']}, activation='softmax'))\")\n",
        "        model.add(Dense(options['num_classes'], activation='softmax'))\n",
        "\n",
        "        return model\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VVw3dAJZgU9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyLRSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "\n",
        "  def __init__(self, initial_learning_rate):\n",
        "    self.initial_learning_rate = initial_learning_rate\n",
        "\n",
        "  def __call__(self, step):\n",
        "     return self.initial_learning_rate / (step + 1)\n"
      ],
      "metadata": {
        "id": "Hr-o06youSPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "filepath = MODEL_CHECKPOINT_PATH + \"model-{epoch:02d}-{val_accuracy:.2f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "num_epochs=10\n",
        "#user_ids =[i for i in range(1,5)]\n",
        "\n",
        "model_options = ModelOptions(\n",
        "    num_classes=len(user_ids),\n",
        "    batch_size=100,\n",
        "    image_height=150,\n",
        "    image_width=150,\n",
        "    num_channels=1,\n",
        "    max_sequence_length=40,\n",
        "    random_shuffle_amount=2\n",
        ")\n",
        "\n",
        "layers_options = {\n",
        "    'depth':5,  # number of convolutional layers\n",
        "    'filters': [32,64,128,256,512],  # number of filters for each conv layer\n",
        "    'kernel_sizes': [(10,10),(5,5), (3,3), (3,3), (1,1 )],  # filter sizes\n",
        "    'strides': [(3, 3),(3, 3),(1, 1),(3, 3),(1, 1)],  # strides for each conv layer\n",
        "    'padding': [ 'same','same','same','same','same' ],  # padding for each conv layer\n",
        "    'conv_activation': 'relu',  # activation function for the convolutional layers\n",
        "    'pooling': [True,True,True,False,False],  # whether to include a pooling layer after each conv layer\n",
        "    'pool_sizes': [(3, 3),(3, 3),(3, 3),(3, 3)],  # sizes of the pooling filters\n",
        "    'pool_strides': [(2, 2),(2, 2),(2, 2),(2, 2),(2, 2)],  # strides for each pooling layer\n",
        "    'fc_layers': 1,  # number of fully connected layers\n",
        "    'fc_units': [256],  # number of units in each fully connected layer\n",
        "    'fc_activation': 'relu',  # activation function for the fully connected layers\n",
        "    'dropout_rate': 0.2,  # dropout rate\n",
        "    'num_classes':model_options.num_classes   # number of classes in the output layer\n",
        "}\n",
        "\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "\n",
        "# If a GPU is available, the TensorFlow should default to it\n",
        "if tf.test.gpu_device_name():\n",
        "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
        "else:\n",
        "    print(\"Please install GPU version of TF\")\n",
        "\n",
        "\n",
        "\n",
        "input_shape = (model_options.image_height,model_options.image_width ,1)\n",
        "num_classes = model_options.num_classes\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UurDRKZLtO8E",
        "outputId": "d8c1fc88-dc91-45c2-bd79-41f9f9caf7e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  0\n",
            "Please install GPU version of TF\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_ids = [random.randint(1,40),random.randint(41,80),random.randint(81,120)]\n",
        "train_gen = LettersGenerator(MODE_TRAIN,user_ids, model_options)\n",
        "valid_gen = LettersGenerator(MODE_VALIDATION, user_ids, model_options)"
      ],
      "metadata": {
        "id": "XqsvkaSpRXqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sm = CustomCNN(input_shape)\n",
        "model = sm.get_model(layers_options)\n",
        "\n",
        "opt =  Adam(learning_rate=1e-3)\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "history = model.fit(train_gen, epochs=num_epochs, batch_size=model_options.batch_size,\n",
        "                   validation_data=valid_gen, verbose=1)\n",
        "#                   validation_data=valid_gen, verbose=1, callbacks=callbacks_list)"
      ],
      "metadata": {
        "id": "Rk8siAndlok8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "history_dict = history.history\n",
        "loss_values = history_dict['loss']\n",
        "val_loss_values = history_dict['val_loss']\n",
        "acc  = history_dict['accuracy']\n",
        "epochs = range(1, len(acc) + 1)\n",
        "plt.clf()\n",
        "plt.plot(epochs, loss_values, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "h-HC5Gj_STiR",
        "outputId": "d0458329-2b0d-4925-dd23-f0d1a87b42a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.clf()\n",
        "acc_values = history_dict['accuracy']\n",
        "val_acc_values = history_dict['val_accuracy']\n",
        "plt.plot(epochs, acc_values, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc_values, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qogd3Eul6YnC",
        "outputId": "e0edb6ea-ecce-41ff-c390-b2f0a8198209",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "from keras import backend as K\n",
        "filepath = MODEL_CHECKPOINT_PATH + 'model-15-0.41.hdf5'\n",
        "# load the model\n",
        "loaded_model = load_model(filepath)\n",
        "K.set_value(loaded_model.optimizer.lr,1e-6)\n",
        "loaded_model.summary()\n",
        "history = loaded_model.fit(train_gen, epochs=num_epochs, batch_size=model_options.batch_size,\n",
        "                    validation_data=valid_gen, verbose=1, callbacks=callbacks_list)"
      ],
      "metadata": {
        "id": "9wcw8a0LmCy7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b7f16e2d-0890-405a-f7d7-a7e864eb53ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_5 (Conv2D)           (None, 35, 35, 96)        11712     \n",
            "                                                                 \n",
            " activation_9 (Activation)   (None, 35, 35, 96)        0         \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 17, 17, 96)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 17, 17, 256)       614656    \n",
            "                                                                 \n",
            " activation_10 (Activation)  (None, 17, 17, 256)       0         \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPooling  (None, 8, 8, 256)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 8, 8, 384)         885120    \n",
            "                                                                 \n",
            " activation_11 (Activation)  (None, 8, 8, 384)         0         \n",
            "                                                                 \n",
            " conv2d_8 (Conv2D)           (None, 8, 8, 384)         1327488   \n",
            "                                                                 \n",
            " activation_12 (Activation)  (None, 8, 8, 384)         0         \n",
            "                                                                 \n",
            " conv2d_9 (Conv2D)           (None, 8, 8, 256)         884992    \n",
            "                                                                 \n",
            " activation_13 (Activation)  (None, 8, 8, 256)         0         \n",
            "                                                                 \n",
            " max_pooling2d_5 (MaxPooling  (None, 3, 3, 256)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 2304)              0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 2048)              4720640   \n",
            "                                                                 \n",
            " activation_14 (Activation)  (None, 2048)              0         \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 1024)              2098176   \n",
            "                                                                 \n",
            " activation_15 (Activation)  (None, 1024)              0         \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 512)               524800    \n",
            "                                                                 \n",
            " activation_16 (Activation)  (None, 512)               0         \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 10)                5130      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 11,072,714\n",
            "Trainable params: 11,072,714\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "750/750 [==============================] - ETA: 0s - loss: 1.6583 - accuracy: 0.4104new generator for 16 anchorFalse\n",
            "new generator for 15 anchorFalse\n",
            "new generator for 17 anchorFalse\n",
            "new generator for 20 anchorFalse\n",
            "new generator for 19 anchorFalse\n",
            "new generator for 13 anchorFalse\n",
            "new generator for 14 anchorFalse\n",
            "new generator for 21 anchorFalse\n",
            "new generator for 12 anchorFalse\n",
            "new generator for 18 anchorFalse\n",
            "\n",
            "Epoch 1: val_accuracy improved from -inf to 0.40385, saving model to /gdrive/MyDrive/deep-learning/final-project/data-sets/model_checkpoints/model-01-0.40.hdf5\n",
            "750/750 [==============================] - 596s 774ms/step - loss: 1.6583 - accuracy: 0.4104 - val_loss: 1.6902 - val_accuracy: 0.4039\n",
            "Epoch 2/100\n",
            "new generator for 16 anchorFalse\n",
            "new generator for 20 anchorFalse\n",
            "new generator for 13 anchorFalse\n",
            "new generator for 14 anchorFalse\n",
            "new generator for 18 anchorFalse\n",
            "new generator for 15 anchorFalse\n",
            "new generator for 19 anchorFalse\n",
            "new generator for 17 anchorFalse\n",
            "new generator for 12 anchorFalse\n",
            "new generator for 21 anchorFalse\n",
            "750/750 [==============================] - ETA: 0s - loss: 1.6433 - accuracy: 0.4170new generator for 15 anchorFalse\n",
            "new generator for 18 anchorFalse\n",
            "new generator for 16 anchorFalse\n",
            "new generator for 17 anchorFalse\n",
            "new generator for 13 anchorFalse\n",
            "new generator for 12 anchorFalse\n",
            "new generator for 20 anchorFalse\n",
            "new generator for 14 anchorFalse\n",
            "new generator for 21 anchorFalse\n",
            "new generator for 19 anchorFalse\n",
            "\n",
            "Epoch 2: val_accuracy improved from 0.40385 to 0.41557, saving model to /gdrive/MyDrive/deep-learning/final-project/data-sets/model_checkpoints/model-02-0.42.hdf5\n",
            "750/750 [==============================] - 583s 778ms/step - loss: 1.6433 - accuracy: 0.4170 - val_loss: 1.6673 - val_accuracy: 0.4156\n",
            "Epoch 3/100\n",
            "new generator for 18 anchorFalse\n",
            "new generator for 20 anchorFalse\n",
            "new generator for 19 anchorFalse\n",
            "new generator for 13 anchorFalse\n",
            "new generator for 16 anchorFalse\n",
            "new generator for 14 anchorFalse\n",
            "new generator for 12 anchorFalse\n",
            "new generator for 15 anchorFalse\n",
            "new generator for 21 anchorFalse\n",
            "new generator for 17 anchorFalse\n",
            "750/750 [==============================] - ETA: 0s - loss: 1.6261 - accuracy: 0.4245new generator for 20 anchorFalse\n",
            "new generator for 12 anchorFalse\n",
            "new generator for 21 anchorFalse\n",
            "new generator for 17 anchorFalse\n",
            "new generator for 15 anchorFalse\n",
            "new generator for 18 anchorFalse\n",
            "new generator for 19 anchorFalse\n",
            "new generator for 13 anchorFalse\n",
            "new generator for 14 anchorFalse\n",
            "new generator for 16 anchorFalse\n",
            "\n",
            "Epoch 3: val_accuracy improved from 0.41557 to 0.41983, saving model to /gdrive/MyDrive/deep-learning/final-project/data-sets/model_checkpoints/model-03-0.42.hdf5\n",
            "750/750 [==============================] - 582s 776ms/step - loss: 1.6261 - accuracy: 0.4245 - val_loss: 1.6675 - val_accuracy: 0.4198\n",
            "Epoch 4/100\n",
            "new generator for 19 anchorFalse\n",
            "new generator for 17 anchorFalse\n",
            "new generator for 20 anchorFalse\n",
            "new generator for 12 anchorFalse\n",
            "new generator for 21 anchorFalse\n",
            "new generator for 13 anchorFalse\n",
            "new generator for 16 anchorFalse\n",
            "new generator for 14 anchorFalse\n",
            "new generator for 15 anchorFalse\n",
            "new generator for 18 anchorFalse\n",
            "750/750 [==============================] - ETA: 0s - loss: 1.6133 - accuracy: 0.4305new generator for 14 anchorFalse\n",
            "new generator for 18 anchorFalse\n",
            "new generator for 21 anchorFalse\n",
            "new generator for 12 anchorFalse\n",
            "new generator for 17 anchorFalse\n",
            "new generator for 16 anchorFalse\n",
            "new generator for 13 anchorFalse\n",
            "new generator for 19 anchorFalse\n",
            "new generator for 20 anchorFalse\n",
            "new generator for 15 anchorFalse\n",
            "\n",
            "Epoch 4: val_accuracy improved from 0.41983 to 0.42001, saving model to /gdrive/MyDrive/deep-learning/final-project/data-sets/model_checkpoints/model-04-0.42.hdf5\n",
            "750/750 [==============================] - 583s 778ms/step - loss: 1.6133 - accuracy: 0.4305 - val_loss: 1.6548 - val_accuracy: 0.4200\n",
            "Epoch 5/100\n",
            "new generator for 15 anchorFalse\n",
            "new generator for 12 anchorFalse\n",
            "new generator for 19 anchorFalse\n",
            "new generator for 17 anchorFalse\n",
            "new generator for 18 anchorFalse\n",
            "new generator for 13 anchorFalse\n",
            "new generator for 20 anchorFalse\n",
            "new generator for 16 anchorFalse\n",
            "new generator for 14 anchorFalse\n",
            "new generator for 21 anchorFalse\n",
            "750/750 [==============================] - ETA: 0s - loss: 1.6043 - accuracy: 0.4312new generator for 15 anchorFalse\n",
            "new generator for 13 anchorFalse\n",
            "new generator for 16 anchorFalse\n",
            "new generator for 12 anchorFalse\n",
            "new generator for 19 anchorFalse\n",
            "new generator for 17 anchorFalse\n",
            "new generator for 20 anchorFalse\n",
            "new generator for 14 anchorFalse\n",
            "new generator for 18 anchorFalse\n",
            "new generator for 21 anchorFalse\n",
            "\n",
            "Epoch 5: val_accuracy improved from 0.42001 to 0.42826, saving model to /gdrive/MyDrive/deep-learning/final-project/data-sets/model_checkpoints/model-05-0.43.hdf5\n",
            "750/750 [==============================] - 582s 776ms/step - loss: 1.6043 - accuracy: 0.4312 - val_loss: 1.6421 - val_accuracy: 0.4283\n",
            "Epoch 6/100\n",
            "new generator for 19 anchorFalse\n",
            "new generator for 12 anchorFalse\n",
            "new generator for 13 anchorFalse\n",
            "new generator for 16 anchorFalse\n",
            "new generator for 18 anchorFalse\n",
            "new generator for 17 anchorFalse\n",
            "new generator for 15 anchorFalse\n",
            "new generator for 14 anchorFalse\n",
            "new generator for 21 anchorFalse\n",
            "new generator for 20 anchorFalse\n",
            "750/750 [==============================] - ETA: 0s - loss: 1.5894 - accuracy: 0.4383new generator for 17 anchorFalse\n",
            "new generator for 19 anchorFalse\n",
            "new generator for 20 anchorFalse\n",
            "new generator for 16 anchorFalse\n",
            "new generator for 15 anchorFalse\n",
            "new generator for 12 anchorFalse\n",
            "new generator for 18 anchorFalse\n",
            "new generator for 14 anchorFalse\n",
            "new generator for 21 anchorFalse\n",
            "new generator for 13 anchorFalse\n",
            "\n",
            "Epoch 6: val_accuracy improved from 0.42826 to 0.43254, saving model to /gdrive/MyDrive/deep-learning/final-project/data-sets/model_checkpoints/model-06-0.43.hdf5\n",
            "750/750 [==============================] - 581s 775ms/step - loss: 1.5894 - accuracy: 0.4383 - val_loss: 1.6341 - val_accuracy: 0.4325\n",
            "Epoch 7/100\n",
            "new generator for 15 anchorFalse\n",
            "new generator for 19 anchorFalse\n",
            "new generator for 21 anchorFalse\n",
            "new generator for 14 anchorFalse\n",
            "new generator for 16 anchorFalse\n",
            "new generator for 18 anchorFalse\n",
            "new generator for 20 anchorFalse\n",
            "new generator for 13 anchorFalse\n",
            "new generator for 17 anchorFalse\n",
            "new generator for 12 anchorFalse\n",
            "750/750 [==============================] - ETA: 0s - loss: 1.5756 - accuracy: 0.4452new generator for 12 anchorFalse\n",
            "new generator for 15 anchorFalse\n",
            "new generator for 17 anchorFalse\n",
            "new generator for 13 anchorFalse\n",
            "new generator for 21 anchorFalse\n",
            "new generator for 16 anchorFalse\n",
            "new generator for 20 anchorFalse\n",
            "new generator for 19 anchorFalse\n",
            "new generator for 18 anchorFalse\n",
            "new generator for 14 anchorFalse\n",
            "\n",
            "Epoch 7: val_accuracy improved from 0.43254 to 0.43534, saving model to /gdrive/MyDrive/deep-learning/final-project/data-sets/model_checkpoints/model-07-0.44.hdf5\n",
            "750/750 [==============================] - 582s 776ms/step - loss: 1.5756 - accuracy: 0.4452 - val_loss: 1.6228 - val_accuracy: 0.4353\n",
            "Epoch 8/100\n",
            "new generator for 17 anchorFalse\n",
            "new generator for 20 anchorFalse\n",
            "new generator for 14 anchorFalse\n",
            "new generator for 19 anchorFalse\n",
            "new generator for 16 anchorFalse\n",
            "new generator for 15 anchorFalse\n",
            "new generator for 13 anchorFalse\n",
            "new generator for 21 anchorFalse\n",
            "new generator for 12 anchorFalse\n",
            "new generator for 18 anchorFalse\n",
            "750/750 [==============================] - ETA: 0s - loss: 1.5674 - accuracy: 0.4479new generator for 18 anchorFalse\n",
            "new generator for 13 anchorFalse\n",
            "new generator for 15 anchorFalse\n",
            "new generator for 21 anchorFalse\n",
            "new generator for 19 anchorFalse\n",
            "new generator for 14 anchorFalse\n",
            "new generator for 20 anchorFalse\n",
            "new generator for 17 anchorFalse\n",
            "new generator for 12 anchorFalse\n",
            "new generator for 16 anchorFalse\n",
            "\n",
            "Epoch 8: val_accuracy improved from 0.43534 to 0.43834, saving model to /gdrive/MyDrive/deep-learning/final-project/data-sets/model_checkpoints/model-08-0.44.hdf5\n",
            "750/750 [==============================] - 581s 775ms/step - loss: 1.5674 - accuracy: 0.4479 - val_loss: 1.6103 - val_accuracy: 0.4383\n",
            "Epoch 9/100\n",
            "new generator for 12 anchorFalse\n",
            "new generator for 17 anchorFalse\n",
            "new generator for 19 anchorFalse\n",
            "new generator for 13 anchorFalse\n",
            "new generator for 18 anchorFalse\n",
            "new generator for 20 anchorFalse\n",
            "new generator for 21 anchorFalse\n",
            "new generator for 16 anchorFalse\n",
            "new generator for 15 anchorFalse\n",
            "new generator for 14 anchorFalse\n",
            "750/750 [==============================] - ETA: 0s - loss: 1.5534 - accuracy: 0.4555new generator for 18 anchorFalse\n",
            "new generator for 15 anchorFalse\n",
            "new generator for 21 anchorFalse\n",
            "new generator for 17 anchorFalse\n",
            "new generator for 19 anchorFalse\n",
            "new generator for 20 anchorFalse\n",
            "new generator for 12 anchorFalse\n",
            "new generator for 16 anchorFalse\n",
            "new generator for 14 anchorFalse\n",
            "new generator for 13 anchorFalse\n",
            "\n",
            "Epoch 9: val_accuracy improved from 0.43834 to 0.44717, saving model to /gdrive/MyDrive/deep-learning/final-project/data-sets/model_checkpoints/model-09-0.45.hdf5\n",
            "750/750 [==============================] - 583s 778ms/step - loss: 1.5534 - accuracy: 0.4555 - val_loss: 1.5986 - val_accuracy: 0.4472\n",
            "Epoch 10/100\n",
            "new generator for 19 anchorFalse\n",
            "new generator for 17 anchorFalse\n",
            "new generator for 13 anchorFalse\n",
            "new generator for 14 anchorFalse\n",
            "new generator for 18 anchorFalse\n",
            "new generator for 15 anchorFalse\n",
            "new generator for 12 anchorFalse\n",
            "new generator for 21 anchorFalse\n",
            "new generator for 16 anchorFalse\n",
            "new generator for 20 anchorFalse\n",
            "750/750 [==============================] - ETA: 0s - loss: 1.5487 - accuracy: 0.4564new generator for 21 anchorFalse\n",
            "new generator for 14 anchorFalse\n",
            "new generator for 12 anchorFalse\n",
            "new generator for 16 anchorFalse\n",
            "new generator for 19 anchorFalse\n",
            "new generator for 18 anchorFalse\n",
            "new generator for 17 anchorFalse\n",
            "new generator for 13 anchorFalse\n",
            "new generator for 20 anchorFalse\n",
            "new generator for 15 anchorFalse\n",
            "\n",
            "Epoch 10: val_accuracy improved from 0.44717 to 0.45012, saving model to /gdrive/MyDrive/deep-learning/final-project/data-sets/model_checkpoints/model-10-0.45.hdf5\n",
            "750/750 [==============================] - 581s 775ms/step - loss: 1.5487 - accuracy: 0.4564 - val_loss: 1.5923 - val_accuracy: 0.4501\n",
            "Epoch 11/100\n",
            "new generator for 21 anchorFalse\n",
            "new generator for 15 anchorFalse\n",
            "new generator for 13 anchorFalse\n",
            "new generator for 16 anchorFalse\n",
            "new generator for 19 anchorFalse\n",
            "new generator for 12 anchorFalse\n",
            "new generator for 20 anchorFalse\n",
            "new generator for 17 anchorFalse\n",
            "new generator for 18 anchorFalse\n",
            "new generator for 14 anchorFalse\n",
            "750/750 [==============================] - ETA: 0s - loss: 1.5392 - accuracy: 0.4596new generator for 13 anchorFalse\n",
            "new generator for 15 anchorFalse\n",
            "new generator for 21 anchorFalse\n",
            "new generator for 17 anchorFalse\n",
            "new generator for 12 anchorFalse\n",
            "new generator for 18 anchorFalse\n",
            "new generator for 19 anchorFalse\n",
            "new generator for 20 anchorFalse\n",
            "new generator for 16 anchorFalse\n",
            "new generator for 14 anchorFalse\n",
            "\n",
            "Epoch 11: val_accuracy improved from 0.45012 to 0.45114, saving model to /gdrive/MyDrive/deep-learning/final-project/data-sets/model_checkpoints/model-11-0.45.hdf5\n",
            "750/750 [==============================] - 581s 775ms/step - loss: 1.5392 - accuracy: 0.4596 - val_loss: 1.5833 - val_accuracy: 0.4511\n",
            "Epoch 12/100\n",
            "new generator for 17 anchorFalse\n",
            "new generator for 21 anchorFalse\n",
            "new generator for 15 anchorFalse\n",
            "new generator for 16 anchorFalse\n",
            "new generator for 20 anchorFalse\n",
            "new generator for 19 anchorFalse\n",
            "new generator for 13 anchorFalse\n",
            "new generator for 14 anchorFalse\n",
            "new generator for 12 anchorFalse\n",
            "new generator for 18 anchorFalse\n",
            "750/750 [==============================] - ETA: 0s - loss: 1.5274 - accuracy: 0.4637new generator for 14 anchorFalse\n",
            "new generator for 18 anchorFalse\n",
            "new generator for 19 anchorFalse\n",
            "new generator for 15 anchorFalse\n",
            "new generator for 13 anchorFalse\n",
            "new generator for 12 anchorFalse\n",
            "new generator for 20 anchorFalse\n",
            "new generator for 17 anchorFalse\n",
            "new generator for 16 anchorFalse\n",
            "new generator for 21 anchorFalse\n",
            "\n",
            "Epoch 12: val_accuracy improved from 0.45114 to 0.45662, saving model to /gdrive/MyDrive/deep-learning/final-project/data-sets/model_checkpoints/model-12-0.46.hdf5\n",
            "750/750 [==============================] - 581s 774ms/step - loss: 1.5274 - accuracy: 0.4637 - val_loss: 1.5745 - val_accuracy: 0.4566\n",
            "Epoch 13/100\n",
            "new generator for 20 anchorFalse\n",
            "new generator for 18 anchorFalse\n",
            "new generator for 17 anchorFalse\n",
            "new generator for 12 anchorFalse\n",
            "new generator for 16 anchorFalse\n",
            "new generator for 19 anchorFalse\n",
            "new generator for 14 anchorFalse\n",
            "new generator for 13 anchorFalse\n",
            "new generator for 15 anchorFalse\n",
            "new generator for 21 anchorFalse\n",
            "750/750 [==============================] - ETA: 0s - loss: 1.5231 - accuracy: 0.4645new generator for 21 anchorFalse\n",
            "new generator for 16 anchorFalse\n",
            "new generator for 18 anchorFalse\n",
            "new generator for 13 anchorFalse\n",
            "new generator for 17 anchorFalse\n",
            "new generator for 14 anchorFalse\n",
            "new generator for 15 anchorFalse\n",
            "new generator for 20 anchorFalse\n",
            "new generator for 19 anchorFalse\n",
            "new generator for 12 anchorFalse\n",
            "\n",
            "Epoch 13: val_accuracy did not improve from 0.45662\n",
            "750/750 [==============================] - 581s 775ms/step - loss: 1.5231 - accuracy: 0.4645 - val_loss: 1.5746 - val_accuracy: 0.4551\n",
            "Epoch 14/100\n",
            "new generator for 21 anchorFalse\n",
            "new generator for 13 anchorFalse\n",
            "new generator for 19 anchorFalse\n",
            "new generator for 20 anchorFalse\n",
            "new generator for 16 anchorFalse\n",
            "new generator for 18 anchorFalse\n",
            "new generator for 14 anchorFalse\n",
            "new generator for 15 anchorFalse\n",
            "new generator for 12 anchorFalse\n",
            "new generator for 17 anchorFalse\n",
            "750/750 [==============================] - ETA: 0s - loss: 1.5156 - accuracy: 0.4695new generator for 13 anchorFalse\n",
            "new generator for 12 anchorFalse\n",
            "new generator for 21 anchorFalse\n",
            "new generator for 17 anchorFalse\n",
            "new generator for 20 anchorFalse\n",
            "new generator for 14 anchorFalse\n",
            "new generator for 19 anchorFalse\n",
            "new generator for 15 anchorFalse\n",
            "new generator for 18 anchorFalse\n",
            "new generator for 16 anchorFalse\n",
            "\n",
            "Epoch 14: val_accuracy improved from 0.45662 to 0.46097, saving model to /gdrive/MyDrive/deep-learning/final-project/data-sets/model_checkpoints/model-14-0.46.hdf5\n",
            "750/750 [==============================] - 583s 777ms/step - loss: 1.5156 - accuracy: 0.4695 - val_loss: 1.5632 - val_accuracy: 0.4610\n",
            "Epoch 15/100\n",
            "new generator for 13 anchorFalse\n",
            "new generator for 21 anchorFalse\n",
            "new generator for 18 anchorFalse\n",
            "new generator for 20 anchorFalse\n",
            "new generator for 16 anchorFalse\n",
            "new generator for 12 anchorFalse\n",
            "new generator for 14 anchorFalse\n",
            "new generator for 15 anchorFalse\n",
            "new generator for 19 anchorFalse\n",
            "new generator for 17 anchorFalse\n",
            "750/750 [==============================] - ETA: 0s - loss: 1.5083 - accuracy: 0.4724new generator for 17 anchorFalse\n",
            "new generator for 18 anchorFalse\n",
            "new generator for 13 anchorFalse\n",
            "new generator for 16 anchorFalse\n",
            "new generator for 12 anchorFalse\n",
            "new generator for 21 anchorFalse\n",
            "new generator for 15 anchorFalse\n",
            "new generator for 20 anchorFalse\n",
            "new generator for 14 anchorFalse\n",
            "new generator for 19 anchorFalse\n",
            "\n",
            "Epoch 15: val_accuracy improved from 0.46097 to 0.46289, saving model to /gdrive/MyDrive/deep-learning/final-project/data-sets/model_checkpoints/model-15-0.46.hdf5\n",
            "750/750 [==============================] - 583s 777ms/step - loss: 1.5083 - accuracy: 0.4724 - val_loss: 1.5528 - val_accuracy: 0.4629\n",
            "Epoch 16/100\n",
            "new generator for 20 anchorFalse\n",
            "new generator for 17 anchorFalse\n",
            "new generator for 16 anchorFalse\n",
            "new generator for 19 anchorFalse\n",
            "new generator for 13 anchorFalse\n",
            "new generator for 21 anchorFalse\n",
            "new generator for 18 anchorFalse\n",
            "new generator for 14 anchorFalse\n",
            "new generator for 15 anchorFalse\n",
            "new generator for 12 anchorFalse\n",
            "647/750 [========================>.....] - ETA: 40s - loss: 1.4958 - accuracy: 0.4768"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-79af77a30ba9>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mloaded_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m history = loaded_model.fit(train_gen, epochs=num_epochs, batch_size=model_options.batch_size,\n\u001b[0m\u001b[1;32m      7\u001b[0m                     validation_data=valid_gen, verbose=1, callbacks=callbacks_list)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1683\u001b[0m                         ):\n\u001b[1;32m   1684\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1685\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1686\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m       (concrete_function,\n\u001b[1;32m    142\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    144\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1755\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1756\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1758\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1759\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    382\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOWaUtejQ9aNf1J5sIOijnO",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}